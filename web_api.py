import os
import time
import uuid
import asyncio
import threading
from astrbot.api.star import Context
from .services.document_service import DocumentService
from .services.kb_service import KnowledgeBaseService
from .config.settings import PluginSettings
from .vector_store.base import Document
from quart import request
from astrbot.dashboard.server import Response
from astrbot.core.utils.astrbot_path import get_astrbot_data_path
from astrbot import logger
from astrbot.core.config.default import VERSION


class KnowledgeBaseWebAPI:
    def __init__(
        self,
        kb_service: KnowledgeBaseService,
        document_service: DocumentService,
        astrbot_context: Context,
        plugin_config: PluginSettings,
    ):
        self.kb_service = kb_service
        self.document_service = document_service
        self.astrbot_context = astrbot_context
        self.plugin_config = plugin_config

        # ä»æœåŠ¡ä¸­è·å–ä¾èµ–
        self.vec_db = self.kb_service.vector_db
        self.user_prefs_handler = self.kb_service.user_prefs_handler
        self.fp = self.document_service.file_parser
        self.text_splitter = self.document_service.text_splitter
        self.tasks = {}
        
        # æ–‡ä»¶å¤„ç†é”ï¼Œé˜²æ­¢å¹¶å‘å†²çª
        self._file_processing_lock = threading.Lock()
        self._temp_file_counter = 0

        if VERSION < "3.5.13":
            raise RuntimeError("AstrBot ç‰ˆæœ¬è¿‡ä½ï¼Œæ— æ³•æ”¯æŒæ­¤æ’ä»¶ï¼Œè¯·å‡çº§ AstrBotã€‚")

        # æ³¨å†ŒAPIç«¯ç‚¹
        self._register_api_endpoints()

    def _register_api_endpoints(self):
        """æ³¨å†Œæ‰€æœ‰APIç«¯ç‚¹ï¼Œå¢å¼ºå®¹é”™æ€§"""
        endpoints = [
            ("/alkaid/kb/create_collection", self.create_collection, ["POST"], "åˆ›å»ºä¸€ä¸ªæ–°çš„çŸ¥è¯†åº“é›†åˆ"),
            ("/alkaid/kb/collections", self.list_collections, ["GET"], "åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“é›†åˆ"),
            ("/alkaid/kb/collection/add_file", self.add_documents, ["POST"], "å‘æŒ‡å®šé›†åˆæ·»åŠ æ–‡æ¡£"),
            ("/alkaid/kb/collection/search", self.search_documents, ["GET"], "æœç´¢æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£"),
            ("/alkaid/kb/collection/delete", self.delete_collection, ["GET"], "åˆ é™¤æŒ‡å®šé›†åˆ"),
            ("/alkaid/kb/collection/documents", self.list_documents, ["GET"], "è·å–é›†åˆä¸­çš„æ–‡æ¡£åˆ—è¡¨"),
            ("/alkaid/kb/collection/stats", self.get_collection_stats, ["GET"], "è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"),
            ("/alkaid/kb/task_status", self.get_task_status, ["GET"], "è·å–å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€"),
        ]
        
        for path, handler, methods, description in endpoints:
            try:
                self.astrbot_context.register_web_api(path, handler, methods, description)
                logger.debug(f"å·²æ³¨å†ŒAPIç«¯ç‚¹: {path}")
            except Exception as e:
                logger.error(f"æ³¨å†ŒAPIç«¯ç‚¹å¤±è´¥ {path}: {e}")

    def _generate_safe_filename(self, original_filename: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶åï¼Œé¿å…å¹¶å‘å†²çª"""
        with self._file_processing_lock:
            self._temp_file_counter += 1
            timestamp = int(time.time() * 1000)
            safe_name = f"{timestamp}_{self._temp_file_counter}_{original_filename}"
            return safe_name

    async def _safe_api_call(self, func, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥ {func.__name__}: {e}", exc_info=True)
            return Response().error(f"æœåŠ¡å†…éƒ¨é”™è¯¯: {str(e)}").__dict__

    async def create_collection(self):
        """
        åˆ›å»ºä¸€ä¸ªæ–°çš„çŸ¥è¯†åº“é›†åˆã€‚
        :param collection_name: é›†åˆåç§°
        :return: åˆ›å»ºç»“æœ
        """
        data = await request.get_json()
        collection_name = data.get("collection_name")
        emoji = data.get("emoji", "ğŸ™‚")
        description = data.get("description", "")
        embedding_provider_id = data.get("embedding_provider_id", None)
        logger.info(f"æ”¶åˆ°åˆ›å»ºçŸ¥è¯†åº“è¯·æ±‚: {collection_name}")
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
        if await self.vec_db.collection_exists(collection_name):
            return Response().error("é›†åˆå·²å­˜åœ¨").__dict__
        if not embedding_provider_id:
            return Response().error("ç¼ºå°‘åµŒå…¥æä¾›å•† ID").__dict__
        try:
            # æ·»åŠ é›†åˆå…ƒæ•°æ®
            metadata = {
                "version": 1,  # metadata é…ç½®ç‰ˆæœ¬
                "emoji": emoji,
                "description": description,
                "created_at": int(time.time()),
                "file_id": f"KBDB_{str(uuid.uuid4())}",  # æ–‡ä»¶ ID
                "origin": "astrbot-webui",
                "embedding_provider_id": embedding_provider_id,  # AstrBot åµŒå…¥æä¾›å•† ID
            }
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            collection_metadata[collection_name] = metadata
            self.user_prefs_handler.user_collection_preferences[
                "collection_metadata"
            ] = collection_metadata
            await self.user_prefs_handler.save_user_preferences()
            # å…¼å®¹æ€§é—®é¢˜ï¼Œcreate_collection æ–¹æ³•æ”¾åœ¨ä¸Šä¸€æ­¥ä¹‹åæ‰§è¡Œã€‚
            await self.vec_db.create_collection(collection_name)
            return Response().ok("é›†åˆåˆ›å»ºæˆåŠŸ").__dict__
        except Exception as e:
            return Response().error(f"åˆ›å»ºé›†åˆå¤±è´¥: {str(e)}").__dict__

    async def list_collections(self):
        """
        åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“é›†åˆã€‚
        :return: é›†åˆåˆ—è¡¨
        """
        logger.info("æ”¶åˆ°åˆ—å‡ºçŸ¥è¯†åº“é›†åˆè¯·æ±‚")
        try:
            collections = await self.vec_db.list_collections()
            result = []
            collections_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            for collection in collections:
                collection_md = collections_metadata.get(collection, {})
                if "embedding_provider_id" in collection_md:
                    p_id = collection_md.get("embedding_provider_id", "")
                    provider = self.astrbot_context.get_provider_by_id(p_id)
                    if provider:
                        collection_md["_embedding_provider_config"] = (
                            provider.provider_config
                        )
                count = await self.vec_db.count_documents(collection)
                result.append(
                    {"collection_name": collection, "count": count, **collection_md}
                )
            return Response().ok(data=result).__dict__
        except Exception as e:
            return Response().error(f"è·å–é›†åˆåˆ—è¡¨å¤±è´¥: {str(e)}").__dict__

    async def add_documents(self):
        """
        å‘æŒ‡å®šé›†åˆæ·»åŠ æ–‡æ¡£ã€‚
        :param collection_name: é›†åˆåç§°
        :param documents: æ–‡æ¡£åˆ—è¡¨
        :return: æ·»åŠ ç»“æœ
        """
        upload_file = (await request.files).get("file")
        collection_name = (await request.form).get("collection_name")
        chunk_size = (await request.form).get("chunk_size", None)
        overlap = (await request.form).get("chunk_overlap", None)

        logger.info(f"æ”¶åˆ°å‘çŸ¥è¯†åº“ '{collection_name}' æ·»åŠ æ–‡ä»¶çš„è¯·æ±‚: {upload_file.filename}")

        if not upload_file or not collection_name:
            return Response().error("ç¼ºå°‘çŸ¥è¯†åº“åç§°").__dict__
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        task_id = f"task_{uuid.uuid4()}"
        self.tasks[task_id] = {"status": "pending", "result": None}
        logger.info(f"åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ {task_id} ç”¨äºå¤„ç†æ–‡ä»¶ {upload_file.filename}")

        asyncio.create_task(
            self._process_file_asynchronously(
                task_id,
                upload_file,
                collection_name,
                chunk_size,
                overlap,
            )
        )

        return Response().ok(data={"task_id": task_id}, message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ã€‚").__dict__

    async def _process_file_asynchronously(
        self, task_id, upload_file, collection_name, chunk_size_str, overlap_str
    ):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼Œå¢å¼ºå®¹é”™æ€§å’Œå¹¶å‘å®‰å…¨æ€§"""
        self.tasks[task_id]["status"] = "running"
        temp_path = None
        
        try:
            logger.info(f"[Task {task_id}] å¼€å§‹å¤„ç†æ–‡ä»¶: {upload_file.filename}")
            
            # å‚æ•°éªŒè¯å’Œè½¬æ¢
            try:
                chunk_size = int(chunk_size_str) if chunk_size_str else self.plugin_config.text_chunk_size
                overlap = int(overlap_str) if overlap_str else self.plugin_config.text_chunk_overlap
            except (ValueError, TypeError) as e:
                raise ValueError(f"æ— æ•ˆçš„åˆ†å—å‚æ•°: {e}")
            
            # ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_dir = os.path.join(get_astrbot_data_path(), "temp")
            os.makedirs(temp_dir, exist_ok=True)
            
            safe_filename = self._generate_safe_filename(upload_file.filename)
            temp_path = os.path.join(temp_dir, safe_filename)
            
            # ä¿å­˜æ–‡ä»¶
            try:
                await upload_file.save(temp_path)
                logger.info(f"[Task {task_id}] æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶è·¯å¾„: {temp_path}")
            except Exception as e:
                raise IOError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

            # è§£ææ–‡ä»¶å†…å®¹
            try:
                content = await self.fp.parse_file_content(temp_path)
                if not content or not content.strip():
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ")
                logger.info(f"[Task {task_id}] æ–‡ä»¶å†…å®¹è§£æå®Œæˆï¼Œé•¿åº¦: {len(content)}")
            except Exception as e:
                raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")

            # æ–‡æœ¬åˆ†å‰²
            try:
                chunks = self.text_splitter.split_text(
                    text=content, chunk_size=chunk_size, overlap=overlap
                )
                if not chunks:
                    raise ValueError("æ–‡æœ¬åˆ†å‰²åæ— æœ‰æ•ˆå†…å®¹")
                logger.info(f"[Task {task_id}] æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
            except Exception as e:
                raise ValueError(f"æ–‡æœ¬åˆ†å‰²å¤±è´¥: {e}")

            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
            if not await self.vec_db.collection_exists(collection_name):
                raise ValueError(f"ç›®æ ‡çŸ¥è¯†åº“ '{collection_name}' ä¸å­˜åœ¨")

            # å‡†å¤‡æ–‡æ¡£
            documents_to_add = [
                Document(
                    text_content=chunk,
                    metadata={
                        "source": upload_file.filename,
                        "user": "astrbot_webui",
                        "upload_time": int(time.time()),
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    },
                )
                for i, chunk in enumerate(chunks)
            ]

            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
            try:
                doc_ids = await self.vec_db.add_documents(collection_name, documents_to_add)
                if not doc_ids:
                    raise ValueError("å‘é‡æ•°æ®åº“è¿”å›ç©ºæ–‡æ¡£IDåˆ—è¡¨")
            except Exception as e:
                raise ValueError(f"æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            
            success_message = f"æˆåŠŸä»æ–‡ä»¶ '{upload_file.filename}' æ·»åŠ  {len(doc_ids)} æ¡çŸ¥è¯†åˆ° '{collection_name}'ã€‚"
            self.tasks[task_id] = {"status": "success", "result": success_message}
            logger.info(f"[Task {task_id}] ä»»åŠ¡æˆåŠŸ: {success_message}")

        except Exception as e:
            error_message = f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            self.tasks[task_id] = {"status": "failed", "result": error_message}
            logger.error(f"[Task {task_id}] ä»»åŠ¡å¤±è´¥: {error_message}", exc_info=True)
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                    logger.info(f"[Task {task_id}] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_path}")
                except Exception as e:
                    logger.warning(f"[Task {task_id}] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    async def search_documents(self):
        """
        æœç´¢æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£ã€‚
        :param collection_name: é›†åˆåç§°
        :param query: æŸ¥è¯¢å­—ç¬¦ä¸²
        :param top_k: è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5
        :return: æœç´¢ç»“æœ
        """
        # ä» URL å‚æ•°ä¸­è·å–æŸ¥è¯¢å‚æ•°
        collection_name = request.args.get("collection_name")
        query = request.args.get("query")
        try:
            top_k = int(request.args.get("top_k", 5))
        except ValueError:
            top_k = 5
        
        logger.info(f"æ”¶åˆ°åœ¨çŸ¥è¯†åº“ '{collection_name}' ä¸­æœç´¢çš„è¯·æ±‚: query='{query}', top_k={top_k}")

        # éªŒè¯å¿…è¦å‚æ•°
        if not collection_name or not query:
            return Response().error("ç¼ºå°‘é›†åˆåç§°æˆ–æŸ¥è¯¢å­—ç¬¦ä¸²").__dict__

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        try:
            # æ‰§è¡Œæœç´¢
            results = await self.vec_db.search(collection_name, query, top_k)

            # æ ¼å¼åŒ–ç»“æœä»¥ä¾¿å‰ç«¯å±•ç¤º
            formatted_results = []
            for i, result in enumerate(results):
                formatted_results.append(
                    {
                        "id": result.document.id,
                        "content": result.document.text_content,
                        "metadata": result.document.metadata,
                        "score": result.score,
                    }
                )
            return Response().ok(data=formatted_results).__dict__
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return Response().error(f"æœç´¢å¤±è´¥: {str(e)}").__dict__

    async def delete_collection(self):
        """
        åˆ é™¤æŒ‡å®šé›†åˆã€‚
        :param collection_name: é›†åˆåç§°
        """
        # ä» URL å‚æ•°ä¸­è·å–æŸ¥è¯¢å‚æ•°
        collection_name = request.args.get("collection_name")
        logger.info(f"æ”¶åˆ°åˆ é™¤çŸ¥è¯†åº“è¯·æ±‚: {collection_name}")

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        try:
            # æ‰§è¡Œåˆ é™¤
            await self.vec_db.delete_collection(collection_name)
            logger.info(f"çŸ¥è¯†åº“ '{collection_name}' åˆ é™¤æˆåŠŸ")
            return Response().ok(f"åˆ é™¤ {collection_name} æˆåŠŸ").__dict__
        except Exception as e:
            logger.error(f"åˆ é™¤å¤±è´¥: {str(e)}")
            return Response().error(f"åˆ é™¤å¤±è´¥: {str(e)}").__dict__

    async def get_task_status(self):
        """
        è·å–å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ã€‚
        :param task_id: ä»»åŠ¡ ID
        :return: ä»»åŠ¡çŠ¶æ€
        """
        task_id = request.args.get("task_id")
        logger.debug(f"æ”¶åˆ°è·å–ä»»åŠ¡çŠ¶æ€è¯·æ±‚: {task_id}")
        if not task_id:
            return Response().error("ç¼ºå°‘ä»»åŠ¡ ID").__dict__

        task_info = self.tasks.get(task_id)
        if not task_info:
            return Response().error("ä»»åŠ¡ä¸å­˜åœ¨").__dict__

        logger.debug(f"ä»»åŠ¡ {task_id} çŠ¶æ€: {task_info}")
        return Response().ok(data=task_info).__dict__

    async def list_documents(self):
        """
        è·å–æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£åˆ—è¡¨
        """
        collection_name = request.args.get("collection_name")
        page = int(request.args.get("page", 1))
        page_size = int(request.args.get("page_size", 20))
        
        logger.info(f"æ”¶åˆ°è·å–æ–‡æ¡£åˆ—è¡¨è¯·æ±‚: collection={collection_name}, page={page}")
        
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
        
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__
            
        try:
            # è®¡ç®—åç§»é‡
            offset = (page - 1) * page_size
            
            # è·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆè¿™é‡Œéœ€è¦å‡è®¾å‘é‡æ•°æ®åº“æ”¯æŒåˆ†é¡µæŸ¥è¯¢ï¼‰
            # ç”±äºåŸå§‹æ¥å£å¯èƒ½ä¸æ”¯æŒåˆ†é¡µï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°
            total_count = await self.vec_db.count_documents(collection_name)
            
            # æ¨¡æ‹Ÿè·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆå®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“çš„å‘é‡æ•°æ®åº“APIï¼‰
            documents = []
            
            # åŸºç¡€æ–‡æ¡£ä¿¡æ¯
            for i in range(min(page_size, total_count - offset)):
                doc_id = f"doc_{offset + i}"
                documents.append({
                    "id": doc_id,
                    "source": "unknown",  # éœ€è¦ä»å…ƒæ•°æ®ä¸­è·å–
                    "chunk_index": i,
                    "created_at": "unknown",
                    "preview": "æ–‡æ¡£é¢„è§ˆå†…å®¹..."[:100] + "..."
                })
            
            result = {
                "documents": documents,
                "total": total_count,
                "page": page,
                "page_size": page_size,
                "total_pages": (total_count + page_size - 1) // page_size
            }
            
            return Response().ok(data=result).__dict__
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            return Response().error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}").__dict__

    async def get_collection_stats(self):
        """
        è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        """
        collection_name = request.args.get("collection_name")
        logger.info(f"æ”¶åˆ°è·å–ç»Ÿè®¡ä¿¡æ¯è¯·æ±‚: {collection_name}")
        
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
        
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__
            
        try:
            # è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            doc_count = await self.vec_db.count_documents(collection_name)
            
            # è·å–é›†åˆå…ƒæ•°æ®
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                ).get(collection_name, {}) if self.user_prefs_handler else {}
            )
            
            # è®¡ç®—å­˜å‚¨å¤§å°ï¼ˆä¼°ç®—ï¼‰
            estimated_size = doc_count * 500  # æ¯ä¸ªæ–‡æ¡£ä¼°ç®—500å­—èŠ‚
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "document_count": doc_count,
                "estimated_size_bytes": estimated_size,
                "estimated_size_human": self._format_bytes(estimated_size),
                "created_at": collection_metadata.get("created_at"),
                "last_modified": collection_metadata.get("last_modified", int(time.time())),
                "description": collection_metadata.get("description", ""),
                "emoji": collection_metadata.get("emoji", "ğŸ“š"),
                "embedding_provider": collection_metadata.get("embedding_provider_id", "unknown")
            }
            
            return Response().ok(data=stats).__dict__
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return Response().error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}").__dict__

    def _format_bytes(self, bytes_size):
        """æ ¼å¼åŒ–å­—èŠ‚å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f}{unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f}TB"
