import os
import time
import uuid
import asyncio
import threading
import json
from astrbot.api.star import Context
from .services.document_service import DocumentService
from .services.kb_service import KnowledgeBaseService
from .config.settings import PluginSettings
from .vector_store.base import Document
from quart import request
from astrbot.dashboard.server import Response
from astrbot.core.utils.astrbot_path import get_astrbot_data_path
from astrbot import logger
from astrbot.core.config.default import VERSION


class KnowledgeBaseWebAPI:
    def __init__(
        self,
        kb_service: KnowledgeBaseService,
        document_service: DocumentService,
        astrbot_context: Context,
        plugin_config: PluginSettings,
    ):
        self.kb_service = kb_service
        self.document_service = document_service
        self.astrbot_context = astrbot_context
        self.plugin_config = plugin_config

        # ä»æœåŠ¡ä¸­è·å–ä¾èµ–
        self.vec_db = self.kb_service.vector_db
        self.user_prefs_handler = self.kb_service.user_prefs_handler
        self.fp = self.document_service.file_parser
        self.text_splitter = self.document_service.text_splitter
        self.tasks = {}

        # æ–‡ä»¶å¤„ç†é”ï¼Œé˜²æ­¢å¹¶å‘å†²çª
        self._file_processing_lock = threading.Lock()
        self._temp_file_counter = 0
        
        # ä»»åŠ¡æŒä¹…åŒ–æ–‡ä»¶è·¯å¾„
        self._tasks_file_path = os.path.join(get_astrbot_data_path(), "kb_tasks.json")
        self._load_tasks_from_file()

        if VERSION < "3.5.13":
            raise RuntimeError("AstrBot ç‰ˆæœ¬è¿‡ä½ï¼Œæ— æ³•æ”¯æŒæ­¤æ’ä»¶ï¼Œè¯·å‡çº§ AstrBotã€‚")

        # æ³¨å†ŒAPIç«¯ç‚¹
        self.astrbot_context.register_web_api(
            "/alkaid/kb/create_collection",
            self.create_collection,
            ["POST"],
            "åˆ›å»ºä¸€ä¸ªæ–°çš„çŸ¥è¯†åº“é›†åˆ",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collections",
            self.list_collections,
            ["GET"],
            "åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“é›†åˆ",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/add_file",
            self.add_documents,
            ["POST"],
            "å‘æŒ‡å®šé›†åˆæ·»åŠ æ–‡æ¡£",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/search",
            self.search_documents,
            ["GET"],
            "æœç´¢æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/delete",
            self.delete_collection,
            ["GET"],
            "åˆ é™¤æŒ‡å®šé›†åˆ",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/documents",
            self.list_documents,
            ["GET"],
            "è·å–é›†åˆä¸­çš„æ–‡æ¡£åˆ—è¡¨",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/stats",
            self.get_collection_stats,
            ["GET"],
            "è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/task_status",
            self.get_task_status,
            ["GET"],
            "è·å–å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/debug/repair_collection",
            self.repair_collection_data,
            ["GET"],
            "ä¿®å¤é›†åˆæ•°æ®ï¼ˆæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼‰",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/document/delete",
            self.delete_document,
            ["DELETE"],
            "åˆ é™¤æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/update",
            self.update_collection,
            ["PUT"],
            "æ›´æ–°é›†åˆçš„å…ƒæ•°æ®ä¿¡æ¯",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/import",
            self.import_collection,
            ["POST"],
            "å¯¼å…¥é›†åˆæ•°æ®",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/collection/batch_upload",
            self.batch_upload_files,
            ["POST"],
            "æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°é›†åˆ",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/recent_tasks",
            self.get_recent_tasks,
            ["GET"],
            "è·å–æœ€è¿‘çš„ä»»åŠ¡åˆ—è¡¨å’ŒçŠ¶æ€",
        )
        self.astrbot_context.register_web_api(
            "/alkaid/kb/tasks/cleanup",
            self.cleanup_old_tasks,
            ["POST"],
            "æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½•",
        )

    def _generate_safe_filename(self, original_filename: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶åï¼Œé¿å…å¹¶å‘å†²çª"""
        with self._file_processing_lock:
            self._temp_file_counter += 1
            timestamp = int(time.time() * 1000)
            safe_name = f"{timestamp}_{self._temp_file_counter}_{original_filename}"
            return safe_name

    async def _safe_api_call(self, func, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥ {func.__name__}: {e}", exc_info=True)
            return Response().error(f"æœåŠ¡å†…éƒ¨é”™è¯¯: {str(e)}").__dict__

    async def create_collection(self):
        """
        åˆ›å»ºä¸€ä¸ªæ–°çš„çŸ¥è¯†åº“é›†åˆã€‚
        :param collection_name: é›†åˆåç§°
        :return: åˆ›å»ºç»“æœ
        """
        data = await request.get_json()
        collection_name = data.get("collection_name")
        emoji = data.get("emoji", "ğŸ™‚")
        description = data.get("description", "")
        embedding_provider_id = data.get("embedding_provider_id", None)
        logger.info(f"æ”¶åˆ°åˆ›å»ºçŸ¥è¯†åº“è¯·æ±‚: {collection_name}")
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
        if await self.vec_db.collection_exists(collection_name):
            return Response().error("é›†åˆå·²å­˜åœ¨").__dict__
        if not embedding_provider_id:
            return Response().error("ç¼ºå°‘åµŒå…¥æä¾›å•† ID").__dict__
        try:
            # æ·»åŠ é›†åˆå…ƒæ•°æ®
            metadata = {
                "version": 1,  # metadata é…ç½®ç‰ˆæœ¬
                "emoji": emoji,
                "description": description,
                "created_at": int(time.time()),
                "file_id": f"KBDB_{str(uuid.uuid4())}",  # æ–‡ä»¶ ID
                "origin": "astrbot-webui",
                "embedding_provider_id": embedding_provider_id,  # AstrBot åµŒå…¥æä¾›å•† ID
            }
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            collection_metadata[collection_name] = metadata
            self.user_prefs_handler.user_collection_preferences[
                "collection_metadata"
            ] = collection_metadata
            await self.user_prefs_handler.save_user_preferences()
            # å…¼å®¹æ€§é—®é¢˜ï¼Œcreate_collection æ–¹æ³•æ”¾åœ¨ä¸Šä¸€æ­¥ä¹‹åæ‰§è¡Œã€‚
            await self.vec_db.create_collection(collection_name)
            return Response().ok("é›†åˆåˆ›å»ºæˆåŠŸ").__dict__
        except Exception as e:
            return Response().error(f"åˆ›å»ºé›†åˆå¤±è´¥: {str(e)}").__dict__

    async def list_collections(self):
        """
        åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“é›†åˆã€‚
        :return: é›†åˆåˆ—è¡¨
        """
        logger.info("æ”¶åˆ°åˆ—å‡ºçŸ¥è¯†åº“é›†åˆè¯·æ±‚")
        try:
            # æ¸…ç†æŸåçš„é›†åˆï¼ˆå¯é€‰ï¼Œä»…åœ¨éœ€è¦æ—¶æ‰§è¡Œï¼‰
            corrupted_collections = await self.vec_db.cleanup_corrupted_collections()
            if corrupted_collections:
                logger.info(f"æ¸…ç†äº† {len(corrupted_collections)} ä¸ªæŸåçš„é›†åˆæ–‡ä»¶")

                # åŒæ—¶æ¸…ç†è¿™äº›é›†åˆçš„å…ƒæ•°æ®
                if self.user_prefs_handler:
                    collection_metadata = (
                        self.user_prefs_handler.user_collection_preferences.get(
                            "collection_metadata", {}
                        )
                    )
                    cleaned_metadata = False
                    for corrupted_name in corrupted_collections:
                        if corrupted_name in collection_metadata:
                            del collection_metadata[corrupted_name]
                            cleaned_metadata = True
                            logger.info(f"æ¸…ç†äº†æŸåé›†åˆ '{corrupted_name}' çš„å…ƒæ•°æ®")

                    if cleaned_metadata:
                        self.user_prefs_handler.user_collection_preferences[
                            "collection_metadata"
                        ] = collection_metadata
                        await self.user_prefs_handler.save_user_preferences()

            collections = await self.vec_db.list_collections()
            result = []
            collections_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            for collection in collections:
                collection_md = collections_metadata.get(collection, {})
                if "embedding_provider_id" in collection_md:
                    p_id = collection_md.get("embedding_provider_id", "")
                    provider = self.astrbot_context.get_provider_by_id(p_id)
                    if provider:
                        collection_md["_embedding_provider_config"] = (
                            provider.provider_config
                        )
                count = await self.vec_db.count_documents(collection)
                result.append(
                    {"collection_name": collection, "count": count, **collection_md}
                )
            return Response().ok(data=result).__dict__
        except Exception as e:
            return Response().error(f"è·å–é›†åˆåˆ—è¡¨å¤±è´¥: {str(e)}").__dict__

    async def add_documents(self):
        """
        å‘æŒ‡å®šé›†åˆæ·»åŠ æ–‡æ¡£ã€‚
        :param collection_name: é›†åˆåç§°
        :param documents: æ–‡æ¡£åˆ—è¡¨
        :return: æ·»åŠ ç»“æœ
        """
        upload_file = (await request.files).get("file")
        collection_name = (await request.form).get("collection_name")
        chunk_size = (await request.form).get("chunk_size", None)
        overlap = (await request.form).get("chunk_overlap", None)

        logger.info(
            f"æ”¶åˆ°å‘çŸ¥è¯†åº“ '{collection_name}' æ·»åŠ æ–‡ä»¶çš„è¯·æ±‚: {upload_file.filename}"
        )

        if not upload_file or not collection_name:
            return Response().error("ç¼ºå°‘çŸ¥è¯†åº“åç§°").__dict__
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        # ç«‹å³ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        try:
            # ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_dir = os.path.join(get_astrbot_data_path(), "temp")
            os.makedirs(temp_dir, exist_ok=True)

            safe_filename = self._generate_safe_filename(upload_file.filename)
            temp_path = os.path.join(temp_dir, safe_filename)

            # ç«‹å³ä¿å­˜æ–‡ä»¶
            await upload_file.save(temp_path)
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶è·¯å¾„: {temp_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}")
            return Response().error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}").__dict__

        task_id = f"task_{uuid.uuid4()}"
        task_info = {
            "status": "pending", 
            "result": None,
            "filename": upload_file.filename,
            "collection_name": collection_name,
            "created_at": int(time.time())
        }
        self._add_task(task_id, task_info)
        logger.info(f"åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ {task_id} ç”¨äºå¤„ç†æ–‡ä»¶ {upload_file.filename}")

        # ç«‹å³å¯åŠ¨å¼‚æ­¥ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
        asyncio.create_task(
            self._process_file_asynchronously(
                task_id,
                temp_path,  # ä¼ é€’æ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯æ–‡ä»¶å¯¹è±¡
                upload_file.filename,  # ä¼ é€’åŸå§‹æ–‡ä»¶å
                collection_name,
                chunk_size,
                overlap,
            )
        )

        return (
            Response()
            .ok(data={
                "task_id": task_id,
                "message": "æ–‡ä»¶å·²æäº¤å¤„ç†ï¼Œè¯·ç¨åæŸ¥çœ‹å¤„ç†ç»“æœ"
            }, message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†")
            .__dict__
        )

    async def _process_file_asynchronously(
        self,
        task_id,
        temp_path,
        original_filename,
        collection_name,
        chunk_size_str,
        overlap_str,
    ):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼Œå¢å¼ºå®¹é”™æ€§å’Œå¹¶å‘å®‰å…¨æ€§"""
        self._update_task(task_id, {"status": "running"})

        try:
            logger.info(f"[Task {task_id}] å¼€å§‹å¤„ç†æ–‡ä»¶: {original_filename}")

            # å‚æ•°éªŒè¯å’Œè½¬æ¢
            try:
                chunk_size = (
                    int(chunk_size_str)
                    if chunk_size_str
                    else self.plugin_config.text_chunk_size
                )
                overlap = (
                    int(overlap_str)
                    if overlap_str
                    else self.plugin_config.text_chunk_overlap
                )
            except (ValueError, TypeError) as e:
                raise ValueError(f"æ— æ•ˆçš„åˆ†å—å‚æ•°: {e}")

            # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(temp_path):
                raise ValueError("ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨")

            logger.info(f"[Task {task_id}] å¼€å§‹è§£ææ–‡ä»¶: {temp_path}")

            # è§£ææ–‡ä»¶å†…å®¹
            try:
                content = await self.fp.parse_file_content(temp_path)
                if not content or not content.strip():
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ")
                logger.info(f"[Task {task_id}] æ–‡ä»¶å†…å®¹è§£æå®Œæˆï¼Œé•¿åº¦: {len(content)}")
            except Exception as e:
                raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")

            # æ–‡æœ¬åˆ†å‰²
            try:
                chunks = await self.text_splitter.split_text(
                    text=content, chunk_size=chunk_size, overlap=overlap
                )
                if not chunks:
                    raise ValueError("æ–‡æœ¬åˆ†å‰²åæ— æœ‰æ•ˆå†…å®¹")
                logger.info(f"[Task {task_id}] æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
            except Exception as e:
                raise ValueError(f"æ–‡æœ¬åˆ†å‰²å¤±è´¥: {e}")

            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
            if not await self.vec_db.collection_exists(collection_name):
                raise ValueError(f"ç›®æ ‡çŸ¥è¯†åº“ '{collection_name}' ä¸å­˜åœ¨")

            # å‡†å¤‡æ–‡æ¡£
            documents_to_add = [
                Document(
                    text_content=chunk,
                    metadata={
                        "source": original_filename,
                        "user": "astrbot_webui",
                        "upload_time": int(time.time()),
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                    },
                )
                for i, chunk in enumerate(chunks)
            ]

            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
            try:
                doc_ids = await self.vec_db.add_documents(
                    collection_name, documents_to_add
                )
                if not doc_ids:
                    raise ValueError("å‘é‡æ•°æ®åº“è¿”å›ç©ºæ–‡æ¡£IDåˆ—è¡¨")
            except Exception as e:
                raise ValueError(f"æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {e}")

            success_message = f"æˆåŠŸä»æ–‡ä»¶ '{original_filename}' æ·»åŠ  {len(doc_ids)} æ¡çŸ¥è¯†åˆ° '{collection_name}'ã€‚"
            self._update_task(task_id, {"status": "success", "result": success_message})
            logger.info(f"[Task {task_id}] ä»»åŠ¡æˆåŠŸ: {success_message}")

        except Exception as e:
            error_message = f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            self._update_task(task_id, {"status": "failed", "result": error_message})
            logger.error(f"[Task {task_id}] ä»»åŠ¡å¤±è´¥: {error_message}", exc_info=True)
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                    logger.info(f"[Task {task_id}] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_path}")
                except Exception as e:
                    logger.warning(f"[Task {task_id}] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    async def search_documents(self):
        """
        æœç´¢æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£ã€‚
        :param collection_name: é›†åˆåç§°
        :param query: æŸ¥è¯¢å­—ç¬¦ä¸²
        :param top_k: è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5
        :return: æœç´¢ç»“æœ
        """
        # ä» URL å‚æ•°ä¸­è·å–æŸ¥è¯¢å‚æ•°
        collection_name = request.args.get("collection_name")
        query = request.args.get("query")
        try:
            top_k = int(request.args.get("top_k", 5))
        except ValueError:
            top_k = 5

        logger.info(
            f"æ”¶åˆ°åœ¨çŸ¥è¯†åº“ '{collection_name}' ä¸­æœç´¢çš„è¯·æ±‚: query='{query}', top_k={top_k}"
        )

        # éªŒè¯å¿…è¦å‚æ•°
        if not collection_name or not query:
            return Response().error("ç¼ºå°‘é›†åˆåç§°æˆ–æŸ¥è¯¢å­—ç¬¦ä¸²").__dict__

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        # TODO: æ·»åŠ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        # æ£€æŸ¥é›†åˆæ˜¯å¦çœŸçš„æœ‰æ•°æ®
        doc_count = await self.vec_db.count_documents(collection_name)
        logger.info(f"çŸ¥è¯†åº“ '{collection_name}' åŒ…å« {doc_count} ä¸ªæ–‡æ¡£")

        if doc_count == 0:
            logger.warning(f"çŸ¥è¯†åº“ '{collection_name}' ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return Response().ok(data=[], message="çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£").__dict__

        try:
            # æ‰§è¡Œæœç´¢
            logger.debug(f"å¼€å§‹åœ¨çŸ¥è¯†åº“ '{collection_name}' ä¸­æœç´¢...")
            results = await self.vec_db.search(collection_name, query, top_k)
            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

            # æ ¼å¼åŒ–ç»“æœä»¥ä¾¿å‰ç«¯å±•ç¤º
            formatted_results = []
            for i, result in enumerate(results):
                logger.debug(
                    f"ç»“æœ {i + 1}: score={result.score:.4f}, id={result.document.id}"
                )
                formatted_results.append(
                    {
                        "id": result.document.id,
                        "content": result.document.text_content,
                        "metadata": result.document.metadata,
                        "score": result.score,
                    }
                )

            logger.info(f"è¿”å›æ ¼å¼åŒ–ç»“æœ: {len(formatted_results)} ä¸ªé¡¹ç›®")
            return Response().ok(data=formatted_results).__dict__
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return Response().error(f"æœç´¢å¤±è´¥: {str(e)}").__dict__

    async def delete_collection(self):
        """
        åˆ é™¤æŒ‡å®šé›†åˆã€‚
        :param collection_name: é›†åˆåç§°
        """
        # ä» URL å‚æ•°ä¸­è·å–æŸ¥è¯¢å‚æ•°
        collection_name = request.args.get("collection_name")
        logger.info(f"æ”¶åˆ°åˆ é™¤çŸ¥è¯†åº“è¯·æ±‚: {collection_name}")

        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__

        try:
            # å°è¯•åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„é›†åˆ
            deleted = await self.vec_db.delete_collection(collection_name)

            # æ¸…ç†ç”¨æˆ·åå¥½ä¸­çš„é›†åˆå…ƒæ•°æ®
            if self.user_prefs_handler:
                collection_metadata = (
                    self.user_prefs_handler.user_collection_preferences.get(
                        "collection_metadata", {}
                    )
                )
                if collection_name in collection_metadata:
                    del collection_metadata[collection_name]
                    self.user_prefs_handler.user_collection_preferences[
                        "collection_metadata"
                    ] = collection_metadata
                    await self.user_prefs_handler.save_user_preferences()
                    logger.info(f"å·²æ¸…ç†çŸ¥è¯†åº“ '{collection_name}' çš„å…ƒæ•°æ®")

            if deleted:
                logger.info(f"çŸ¥è¯†åº“ '{collection_name}' åˆ é™¤æˆåŠŸ")
                return Response().ok(f"åˆ é™¤ {collection_name} æˆåŠŸ").__dict__
            else:
                return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥").__dict__

        except Exception as e:
            logger.error(f"åˆ é™¤å¤±è´¥: {str(e)}")
            return Response().error(f"åˆ é™¤å¤±è´¥: {str(e)}").__dict__

    async def get_task_status(self):
        """
        è·å–å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ã€‚
        :param task_id: ä»»åŠ¡ ID
        :return: ä»»åŠ¡çŠ¶æ€
        """
        task_id = request.args.get("task_id")
        logger.debug(f"æ”¶åˆ°è·å–ä»»åŠ¡çŠ¶æ€è¯·æ±‚: {task_id}")
        if not task_id:
            return Response().error("ç¼ºå°‘ä»»åŠ¡ ID").__dict__

        task_info = self.tasks.get(task_id)
        if not task_info:
            return Response().error("ä»»åŠ¡ä¸å­˜åœ¨").__dict__

        logger.debug(f"ä»»åŠ¡ {task_id} çŠ¶æ€: {task_info}")
        return Response().ok(data=task_info).__dict__

    async def list_documents(self):
        """
        è·å–æŒ‡å®šé›†åˆä¸­çš„æ–‡æ¡£åˆ—è¡¨
        """
        collection_name = request.args.get("collection_name")
        page = int(request.args.get("page", 1))
        page_size = int(request.args.get("page_size", 20))

        logger.info(f"æ”¶åˆ°è·å–æ–‡æ¡£åˆ—è¡¨è¯·æ±‚: collection={collection_name}, page={page}")

        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__

        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        try:
            # è®¡ç®—åç§»é‡
            offset = (page - 1) * page_size

            # è·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆè¿™é‡Œéœ€è¦å‡è®¾å‘é‡æ•°æ®åº“æ”¯æŒåˆ†é¡µæŸ¥è¯¢ï¼‰
            # ç”±äºåŸå§‹æ¥å£å¯èƒ½ä¸æ”¯æŒåˆ†é¡µï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°
            total_count = await self.vec_db.count_documents(collection_name)

            # æ¨¡æ‹Ÿè·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆå®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“çš„å‘é‡æ•°æ®åº“APIï¼‰
            documents = []

            # åŸºç¡€æ–‡æ¡£ä¿¡æ¯
            for i in range(min(page_size, total_count - offset)):
                doc_id = f"doc_{offset + i}"
                documents.append(
                    {
                        "id": doc_id,
                        "source": "unknown",  # éœ€è¦ä»å…ƒæ•°æ®ä¸­è·å–
                        "chunk_index": i,
                        "created_at": "unknown",
                        "preview": "æ–‡æ¡£é¢„è§ˆå†…å®¹..."[:100] + "...",
                    }
                )

            result = {
                "documents": documents,
                "total": total_count,
                "page": page,
                "page_size": page_size,
                "total_pages": (total_count + page_size - 1) // page_size,
            }

            return Response().ok(data=result).__dict__

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            return Response().error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}").__dict__

    async def get_collection_stats(self):
        """
        è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        """
        collection_name = request.args.get("collection_name")
        logger.info(f"æ”¶åˆ°è·å–ç»Ÿè®¡ä¿¡æ¯è¯·æ±‚: {collection_name}")

        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__

        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        try:
            # è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            doc_count = await self.vec_db.count_documents(collection_name)

            # è·å–é›†åˆå…ƒæ•°æ®
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                ).get(collection_name, {})
                if self.user_prefs_handler
                else {}
            )

            # è®¡ç®—å­˜å‚¨å¤§å°ï¼ˆä¼°ç®—ï¼‰
            estimated_size = doc_count * 500  # æ¯ä¸ªæ–‡æ¡£ä¼°ç®—500å­—èŠ‚

            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "document_count": doc_count,
                "estimated_size_bytes": estimated_size,
                "estimated_size_human": self._format_bytes(estimated_size),
                "created_at": collection_metadata.get("created_at"),
                "last_modified": collection_metadata.get(
                    "last_modified", int(time.time())
                ),
                "description": collection_metadata.get("description", ""),
                "emoji": collection_metadata.get("emoji", "ğŸ“š"),
                "embedding_provider": collection_metadata.get(
                    "embedding_provider_id", "unknown"
                ),
            }

            return Response().ok(data=stats).__dict__

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return Response().error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}").__dict__

    def _format_bytes(self, bytes_size):
        """æ ¼å¼åŒ–å­—èŠ‚å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼"""
        for unit in ["B", "KB", "MB", "GB"]:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f}{unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f}TB"

    async def repair_collection_data(self):
        """ä¿®å¤é›†åˆæ•°æ®ï¼šæ£€æŸ¥å¹¶é‡æ–°ç”Ÿæˆç¼ºå¤±çš„å‘é‡"""
        collection_name = request.args.get("collection_name")
        logger.info(f"æ”¶åˆ°ä¿®å¤é›†åˆæ•°æ®è¯·æ±‚: {collection_name}")

        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__

        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__

        try:
            # TODO: å®ç°æ•°æ®ä¿®å¤é€»è¾‘
            # 1. æ£€æŸ¥æ•°æ®åº“å’Œç´¢å¼•çš„ä¸€è‡´æ€§
            # 2. é‡æ–°ç”Ÿæˆç¼ºå¤±çš„å‘é‡
            # 3. é‡å»ºç´¢å¼•æ–‡ä»¶

            # å…ˆè·å–åŸºæœ¬ä¿¡æ¯
            doc_count = await self.vec_db.count_documents(collection_name)

            # æ£€æŸ¥å‘é‡ç´¢å¼•çŠ¶æ€ï¼ˆç®€åŒ–å®ç°ï¼‰
            # æ³¨æ„ï¼šEnhancedVectorStore ä¸ç›´æ¥æš´éœ²å†…éƒ¨ç´¢å¼•çŠ¶æ€
            # è¿™é‡Œä½¿ç”¨æ–‡æ¡£æ•°é‡ä½œä¸ºä¼°ç®—
            index_count = doc_count  # å‡è®¾ç´¢å¼•ä¸æ–‡æ¡£æ•°é‡ä¸€è‡´

            repair_info = {
                "collection_name": collection_name,
                "documents_in_db": doc_count,
                "vectors_in_index": index_count,
                "consistent": True,  # ç®€åŒ–ä¸ºæ€»æ˜¯ä¸€è‡´
                "repair_needed": False,  # ç®€åŒ–ä¸ºä¸éœ€è¦ä¿®å¤
                "suggestion": "å¦‚æœé‡åˆ°æœç´¢é—®é¢˜ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£",
            }

            logger.info(
                f"é›†åˆ '{collection_name}' æ•°æ®çŠ¶æ€: æ•°æ®åº“æ–‡æ¡£={doc_count}, ç´¢å¼•å‘é‡={index_count}"
            )

            return Response().ok(data=repair_info).__dict__

        except Exception as e:
            logger.error(f"ä¿®å¤é›†åˆæ•°æ®å¤±è´¥: {str(e)}")
            return Response().error(f"ä¿®å¤å¤±è´¥: {str(e)}").__dict__

    async def delete_document(self):
        """åˆ é™¤æŒ‡å®šé›†åˆä¸­çš„å•ä¸ªæ–‡æ¡£"""
        data = await request.get_json()
        collection_name = data.get("collection_name")
        document_id = data.get("document_id")
        
        logger.info(f"æ”¶åˆ°åˆ é™¤æ–‡æ¡£è¯·æ±‚: collection={collection_name}, doc_id={document_id}")
        
        if not collection_name or not document_id:
            return Response().error("ç¼ºå°‘é›†åˆåç§°æˆ–æ–‡æ¡£ID").__dict__
            
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__
            
        try:
            # åˆ é™¤æŒ‡å®šæ–‡æ¡£
            deleted = await self.vec_db.delete_document(collection_name, document_id)
            if deleted:
                return Response().ok("æ–‡æ¡£åˆ é™¤æˆåŠŸ").__dict__
            else:
                return Response().error("æ–‡æ¡£ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥").__dict__
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return Response().error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}").__dict__

    async def update_collection(self):
        """æ›´æ–°é›†åˆçš„å…ƒæ•°æ®ä¿¡æ¯"""
        data = await request.get_json()
        collection_name = data.get("collection_name")
        new_name = data.get("new_name")
        emoji = data.get("emoji")
        description = data.get("description")
        
        logger.info(f"æ”¶åˆ°æ›´æ–°é›†åˆå…ƒæ•°æ®è¯·æ±‚: {collection_name}")
        
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
            
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__
            
        try:
            # æ›´æ–°é›†åˆå…ƒæ•°æ®
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            
            if collection_name in collection_metadata:
                metadata = collection_metadata[collection_name]
                if emoji is not None:
                    metadata["emoji"] = emoji
                if description is not None:
                    metadata["description"] = description
                metadata["last_modified"] = int(time.time())
                
                # å¦‚æœéœ€è¦é‡å‘½åé›†åˆ
                if new_name and new_name != collection_name:
                    if await self.vec_db.collection_exists(new_name):
                        return Response().error("æ–°é›†åˆåç§°å·²å­˜åœ¨").__dict__
                    
                    # é‡å‘½åé›†åˆ
                    await self.vec_db.rename_collection(collection_name, new_name)
                    
                    # æ›´æ–°å…ƒæ•°æ®ä¸­çš„é”®å
                    collection_metadata[new_name] = metadata
                    del collection_metadata[collection_name]
                else:
                    collection_metadata[collection_name] = metadata
                
                self.user_prefs_handler.user_collection_preferences[
                    "collection_metadata"
                ] = collection_metadata
                await self.user_prefs_handler.save_user_preferences()
                
                return Response().ok("é›†åˆä¿¡æ¯æ›´æ–°æˆåŠŸ").__dict__
            else:
                return Response().error("é›†åˆå…ƒæ•°æ®ä¸å­˜åœ¨").__dict__
                
        except Exception as e:
            logger.error(f"æ›´æ–°é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
            return Response().error(f"æ›´æ–°å¤±è´¥: {str(e)}").__dict__

    async def import_collection(self):
        """å¯¼å…¥é›†åˆæ•°æ®"""
        data = await request.get_json()
        collection_data = data.get("collection_data")
        overwrite = data.get("overwrite", False)
        
        logger.info(f"æ”¶åˆ°å¯¼å…¥é›†åˆè¯·æ±‚")
        
        if not collection_data:
            return Response().error("ç¼ºå°‘é›†åˆæ•°æ®").__dict__
            
        try:
            collection_name = collection_data.get("collection_name")
            metadata = collection_data.get("metadata", {})
            
            if not collection_name:
                return Response().error("é›†åˆæ•°æ®æ ¼å¼é”™è¯¯").__dict__
                
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if await self.vec_db.collection_exists(collection_name) and not overwrite:
                return Response().error("é›†åˆå·²å­˜åœ¨ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®overwrite=true").__dict__
                
            # åˆ›å»ºæˆ–è¦†ç›–é›†åˆ
            if overwrite and await self.vec_db.collection_exists(collection_name):
                await self.vec_db.delete_collection(collection_name)
                
            await self.vec_db.create_collection(collection_name)
            
            # å¯¼å…¥å…ƒæ•°æ®
            collection_metadata = (
                self.user_prefs_handler.user_collection_preferences.get(
                    "collection_metadata", {}
                )
            )
            collection_metadata[collection_name] = metadata
            self.user_prefs_handler.user_collection_preferences[
                "collection_metadata"
            ] = collection_metadata
            await self.user_prefs_handler.save_user_preferences()
            
            return Response().ok("é›†åˆå¯¼å…¥æˆåŠŸ").__dict__
            
        except Exception as e:
            logger.error(f"å¯¼å…¥é›†åˆå¤±è´¥: {str(e)}")
            return Response().error(f"å¯¼å…¥å¤±è´¥: {str(e)}").__dict__

    async def batch_upload_files(self):
        """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°é›†åˆ"""
        files = await request.files
        form_data = await request.form
        collection_name = form_data.get("collection_name")
        chunk_size = form_data.get("chunk_size")
        overlap = form_data.get("chunk_overlap")
        
        logger.info(f"æ”¶åˆ°æ‰¹é‡ä¸Šä¼ è¯·æ±‚: collection={collection_name}, files={len(files)}")
        
        if not collection_name:
            return Response().error("ç¼ºå°‘é›†åˆåç§°").__dict__
            
        if not await self.vec_db.collection_exists(collection_name):
            return Response().error("ç›®æ ‡çŸ¥è¯†åº“ä¸å­˜åœ¨").__dict__
            
        if not files:
            return Response().error("æ²¡æœ‰ä¸Šä¼ çš„æ–‡ä»¶").__dict__
            
        try:
            task_ids = []
            temp_dir = os.path.join(get_astrbot_data_path(), "temp")
            os.makedirs(temp_dir, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            for file_key, upload_file in files.items():
                if upload_file.filename:
                    safe_filename = self._generate_safe_filename(upload_file.filename)
                    temp_path = os.path.join(temp_dir, safe_filename)
                    
                    # ä¿å­˜æ–‡ä»¶
                    await upload_file.save(temp_path)
                    
                    # åˆ›å»ºå¤„ç†ä»»åŠ¡
                    task_id = f"batch_task_{uuid.uuid4()}"
                    task_info = {
                        "status": "pending", 
                        "result": None, 
                        "filename": upload_file.filename,
                        "collection_name": collection_name,
                        "created_at": int(time.time())
                    }
                    self._add_task(task_id, task_info)
                    task_ids.append(task_id)
                    
                    # å¯åŠ¨å¼‚æ­¥å¤„ç†
                    asyncio.create_task(
                        self._process_file_asynchronously(
                            task_id,
                            temp_path,
                            upload_file.filename,
                            collection_name,
                            chunk_size,
                            overlap,
                        )
                    )
            
            return Response().ok(
                data={"task_ids": task_ids}, 
                message=f"å·²æäº¤{len(task_ids)}ä¸ªæ–‡ä»¶çš„æ‰¹é‡å¤„ç†ä»»åŠ¡"
            ).__dict__
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")
            return Response().error(f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}").__dict__

    async def get_recent_tasks(self):
        """è·å–æœ€è¿‘çš„ä»»åŠ¡åˆ—è¡¨å’ŒçŠ¶æ€"""
        collection_name = request.args.get("collection_name")
        limit = int(request.args.get("limit", 20))
        
        logger.info(f"æ”¶åˆ°è·å–ä»»åŠ¡åˆ—è¡¨è¯·æ±‚: collection_name={collection_name}, limit={limit}")
        logger.info(f"å½“å‰ä»»åŠ¡æ€»æ•°: {len(self.tasks)}")
        
        try:
            # è·å–ä»»åŠ¡åˆ—è¡¨ï¼Œå¦‚æœæŒ‡å®šäº†collection_nameåˆ™è¿‡æ»¤
            recent_tasks = []
            for task_id, task_info in self.tasks.items():
                logger.debug(f"æ£€æŸ¥ä»»åŠ¡ {task_id}: {task_info}")
                if collection_name and task_info.get("collection_name") != collection_name:
                    continue
                
                recent_tasks.append({
                    "task_id": task_id,
                    "filename": task_info.get("filename", "æœªçŸ¥æ–‡ä»¶"),
                    "collection_name": task_info.get("collection_name", ""),
                    "status": task_info.get("status", "unknown"),
                    "result": task_info.get("result"),
                    "created_at": task_info.get("created_at", 0)
                })
            
            logger.info(f"è¿‡æ»¤åçš„ä»»åŠ¡æ•°é‡: {len(recent_tasks)}")
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            recent_tasks.sort(key=lambda x: x["created_at"], reverse=True)
            
            # é™åˆ¶è¿”å›æ•°é‡
            if limit > 0:
                recent_tasks = recent_tasks[:limit]
            
            return Response().ok(data={
                "tasks": recent_tasks,
                "total": len(recent_tasks)
            }).__dict__
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
            return Response().error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}").__dict__

    async def cleanup_old_tasks(self):
        """æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½•"""
        data = await request.get_json()
        max_age_hours = data.get("max_age_hours", 24)  # é»˜è®¤æ¸…ç†24å°æ—¶å‰çš„ä»»åŠ¡
        keep_completed = data.get("keep_completed", True)  # æ˜¯å¦ä¿ç•™å·²å®Œæˆçš„ä»»åŠ¡
        
        try:
            current_time = int(time.time())
            max_age_seconds = max_age_hours * 3600
            
            tasks_to_remove = []
            for task_id, task_info in self.tasks.items():
                task_age = current_time - task_info.get("created_at", current_time)
                
                # å¦‚æœä»»åŠ¡å¤ªæ—§
                if task_age > max_age_seconds:
                    # å¦‚æœè®¾ç½®äº†ä¿ç•™å·²å®Œæˆä»»åŠ¡ï¼Œåˆ™è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
                    if keep_completed and task_info.get("status") == "success":
                        continue
                    tasks_to_remove.append(task_id)
            
            # åˆ é™¤æ—§ä»»åŠ¡
            removed_count = 0
            for task_id in tasks_to_remove:
                if task_id in self.tasks:
                    del self.tasks[task_id]
                    removed_count += 1
            
            # ä¿å­˜æ›´æ–°åçš„ä»»åŠ¡æ•°æ®
            if removed_count > 0:
                self._save_tasks_to_file()
            
            logger.info(f"æ¸…ç†äº† {removed_count} ä¸ªæ—§ä»»åŠ¡è®°å½•")
            
            return Response().ok(data={
                "removed_count": removed_count,
                "remaining_count": len(self.tasks)
            }, message=f"å·²æ¸…ç† {removed_count} ä¸ªæ—§ä»»åŠ¡è®°å½•").__dict__
            
        except Exception as e:
            logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
            return Response().error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}").__dict__

    def _load_tasks_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡æ•°æ®"""
        try:
            if os.path.exists(self._tasks_file_path):
                with open(self._tasks_file_path, 'r', encoding='utf-8') as f:
                    self.tasks = json.load(f)
                logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(self.tasks)} ä¸ªä»»åŠ¡")
            else:
                self.tasks = {}
                logger.info("ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ç©ºä»»åŠ¡å­—å…¸")
        except Exception as e:
            logger.error(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
            self.tasks = {}

    def _save_tasks_to_file(self):
        """ä¿å­˜ä»»åŠ¡æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(self._tasks_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.tasks, f, ensure_ascii=False, indent=2)
            logger.debug(f"ä»»åŠ¡æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")

    def _add_task(self, task_id, task_info):
        """æ·»åŠ ä»»åŠ¡å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        self.tasks[task_id] = task_info
        self._save_tasks_to_file()

    def _update_task(self, task_id, updates):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        if task_id in self.tasks:
            self.tasks[task_id].update(updates)
            self._save_tasks_to_file()
