import os
import asyncio
import json
import gc
import time
import psutil  # ç”¨äºå†…å­˜ç›‘æ§
from typing import List, Dict, Tuple, Set, Optional

# å¼•å…¥ cachetools å’Œ Lock, Set, Optional
try:
    from cachetools import LRUCache
except ImportError:
    raise ImportError("Please install cachetools: pip install cachetools")

from .base import (
    VectorDBBase,
    Document,
    ProcessingBatch,
    DEFAULT_BATCH_SIZE,
)
from astrbot.api import logger
from astrbot.core.db.vec_db.faiss_impl import FaissVecDB
from astrbot.core.provider.provider import EmbeddingProvider
from ..utils.embedding import EmbeddingSolutionHelper
from .faiss_store import FaissStore as OldFaissStore

# å®šä¹‰é»˜è®¤çš„ç¼“å­˜å¤§å°
DEFAULT_MAX_CACHE_SIZE = 3
# å®šä¹‰å†…å­˜ç®¡ç†ç›¸å…³å¸¸é‡
DEFAULT_MEMORY_BATCH_SIZE = 50  # å†…å­˜ä¸­åŒæ—¶å¤„ç†çš„æ–‡æ¡£æ•°é‡
MAX_DOCUMENTS_WARNING_THRESHOLD = 5000  # å¤§æ–‡ä»¶è­¦å‘Šé˜ˆå€¼
# é«˜æ•ˆå¤„ç†ç›¸å…³å¸¸é‡
OPTIMAL_CONCURRENT_TASKS = 3  # æœ€ä¼˜å¹¶å‘ä»»åŠ¡æ•°(åŸºå‡†å€¼)
EMBEDDING_BATCH_SIZE = 15  # embeddingæ‰¹é‡å¤„ç†å¤§å°
MEMORY_CHECK_INTERVAL = 50  # æ¯å¤„ç†å¤šå°‘æ–‡æ¡£æ£€æŸ¥ä¸€æ¬¡å†…å­˜
# åŠ¨æ€å†…å­˜ç®¡ç†å¸¸é‡
DEFAULT_MAX_MEMORY_MB = 2048  # é»˜è®¤æœ€å¤§å†…å­˜é™åˆ¶(MB)
MIN_CONCURRENT_TASKS = 2  # æœ€å°å¹¶å‘ä»»åŠ¡æ•°
MAX_CONCURRENT_TASKS = 12  # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
MEMORY_SAFETY_FACTOR = 0.7  # å¯ç”¨å†…å­˜çš„å®‰å…¨ä½¿ç”¨æ¯”ä¾‹
# æµå¼å¹¶å‘å¤„ç†å¸¸é‡
DEFAULT_RPM_LIMIT = 2000  # é»˜è®¤æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
EMBEDDING_RETRY_DELAY = 1.0  # é™åˆ¶é‡è¯•å»¶è¿Ÿ(ç§’)
EMBEDDING_MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
# æ›´ç²¾ç¡®çš„APIé™åˆ¶é”™è¯¯æ£€æµ‹
RATE_LIMIT_KEYWORDS = [
    # é€šç”¨é™åˆ¶é”™è¯¯
    "rate limit", "rate_limit", "too many requests", "quota", "429", 
    "rate_limit_exceeded", "throttle", "throttling", "exceeded",
    # OpenAIç›¸å…³
    "rate limit exceeded", "requests per minute", "rpm", "tpm",
    "rate limited", "quota exceeded", "usage quota",
    # å…¶ä»–APIæä¾›å•†
    "concurrency limit", "request limit", "api limit",
    "frequency limit", "call limit", "requests exceeded",
    # HTTPçŠ¶æ€ç å’Œå“åº”
    "http 429", "status 429", "429 too many",
    # ä¸­æ–‡é”™è¯¯ä¿¡æ¯
    "è¯·æ±‚è¿‡äºé¢‘ç¹", "è®¿é—®é¢‘ç‡é™åˆ¶", "è¶…å‡ºé…é¢", "è¯·æ±‚é™åˆ¶"
]
# APIé™åˆ¶é”™è¯¯çš„è¯¦ç»†åˆ†ç±»
CRITICAL_RATE_LIMIT_KEYWORDS = [
    "429", "rate limit exceeded", "quota exceeded", "rpm", "tpm"
]


def _is_rate_limit_error(error: Exception) -> tuple[bool, bool]:
    """
    æ™ºèƒ½æ£€æµ‹æ˜¯å¦ä¸ºAPIé™åˆ¶é”™è¯¯
    è¿”å›: (is_rate_limit, is_critical)
    - is_rate_limit: æ˜¯å¦ä¸ºé™åˆ¶é”™è¯¯
    - is_critical: æ˜¯å¦ä¸ºä¸¥é‡é™åˆ¶é”™è¯¯ï¼ˆéœ€è¦ç«‹å³å¤§å¹…å‡å°‘å¹¶å‘ï¼‰
    """
    error_str = str(error).lower()
    error_type = type(error).__name__.lower()
    
    # æ£€æŸ¥é”™è¯¯æ¶ˆæ¯
    is_rate_limit = any(keyword in error_str for keyword in RATE_LIMIT_KEYWORDS)
    is_critical = any(keyword in error_str for keyword in CRITICAL_RATE_LIMIT_KEYWORDS)
    
    # æ£€æŸ¥å¼‚å¸¸ç±»å‹
    if not is_rate_limit:
        rate_limit_types = [
            "ratelimiterror", "quotaerror", "throttleerror", 
            "httperror", "apierror", "requestsexception"
        ]
        is_rate_limit = any(rate_type in error_type for rate_type in rate_limit_types)
    
    # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(error, 'status_code'):
        if error.status_code == 429:
            is_rate_limit = True
            is_critical = True
    
    return is_rate_limit, is_critical


def _check_pickle_file(file_path: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º Pickle æ ¼å¼"""
    try:
        if not os.path.exists(file_path):
            return False
        with open(file_path, "rb") as f:
            magic = f.read(2)
            # å…¼å®¹python3.8ä¹‹å‰çš„pickleåè®®ç‰ˆæœ¬
            return magic in [b"\x80\x04", b"\x80\x03", b"\x80\x02"]
    except Exception:
        return False


def _get_memory_usage_mb() -> float:
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨é‡(MB)"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        return memory_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
    except Exception:
        return 0.0


def _get_system_memory_info() -> Tuple[float, float, float]:
    """
    è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
    è¿”å›: (æ€»å†…å­˜MB, å¯ç”¨å†…å­˜MB, å†…å­˜ä½¿ç”¨ç‡%)
    """
    try:
        memory = psutil.virtual_memory()
        total_mb = memory.total / 1024 / 1024
        available_mb = memory.available / 1024 / 1024
        usage_percent = memory.percent
        return total_mb, available_mb, usage_percent
    except Exception:
        return 0.0, 0.0, 0.0


def _calculate_optimal_concurrency(max_memory_limit_mb: float = DEFAULT_MAX_MEMORY_MB) -> Tuple[int, float, str]:
    """
    åŸºäºç³»ç»Ÿå†…å­˜åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘ä»»åŠ¡æ•°
    
    Args:
        max_memory_limit_mb: ç”¨æˆ·è®¾ç½®çš„æœ€å¤§å†…å­˜é™åˆ¶(MB)
    
    Returns:
        (optimal_tasks, memory_limit_mb, decision_reason)
    """
    try:
        total_mb, available_mb, usage_percent = _get_system_memory_info()
        current_process_mb = _get_memory_usage_mb()
        
        # è®¡ç®—å¯ç”¨å†…å­˜çš„70%
        safe_available_mb = available_mb * MEMORY_SAFETY_FACTOR
        
        # åœ¨ç”¨æˆ·é™åˆ¶å’Œå®‰å…¨å¯ç”¨å†…å­˜ä¹‹é—´é€‰æ‹©è¾ƒå°è€…
        effective_limit_mb = min(max_memory_limit_mb, safe_available_mb)
        
        # ä¼°ç®—æ¯ä¸ªå¹¶å‘ä»»åŠ¡çš„å†…å­˜å¼€é”€(åŸºäºç»éªŒå€¼)
        estimated_task_memory_mb = 50  # æ¯ä¸ªä»»åŠ¡å¤§çº¦50MBå†…å­˜å¼€é”€
        
        # è®¡ç®—å¯æ”¯æŒçš„å¹¶å‘ä»»åŠ¡æ•°
        max_tasks_by_memory = max(1, int(effective_limit_mb / estimated_task_memory_mb))
        
        # åº”ç”¨å¹¶å‘ä»»åŠ¡æ•°é™åˆ¶
        optimal_tasks = min(max_tasks_by_memory, MAX_CONCURRENT_TASKS)
        optimal_tasks = max(optimal_tasks, MIN_CONCURRENT_TASKS)
        
        # ç”Ÿæˆå†³ç­–åŸå› 
        if effective_limit_mb == max_memory_limit_mb:
            reason = f"ç”¨æˆ·é™åˆ¶({max_memory_limit_mb:.0f}MB)"
        else:
            reason = f"ç³»ç»Ÿå¯ç”¨({safe_available_mb:.0f}MB)"
        
        logger.debug(f"[å†…å­˜åˆ†æ] ç³»ç»Ÿå†…å­˜: {total_mb:.0f}MB, å¯ç”¨: {available_mb:.0f}MB ({100-usage_percent:.1f}%ç©ºé—²), "
                    f"å½“å‰è¿›ç¨‹: {current_process_mb:.0f}MB, æœ‰æ•ˆé™åˆ¶: {effective_limit_mb:.0f}MB({reason}), "
                    f"è®¡ç®—å¹¶å‘æ•°: {optimal_tasks}")
        
        return optimal_tasks, effective_limit_mb, reason
        
    except Exception as e:
        logger.warning(f"[å†…å­˜åˆ†æ] å†…å­˜åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¹¶å‘æ•°: {e}")
        return OPTIMAL_CONCURRENT_TASKS, max_memory_limit_mb, "å†…å­˜åˆ†æå¤±è´¥"


def _should_trigger_gc(processed_count: int, memory_threshold_mb: float = 1000) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘åƒåœ¾å›æ”¶"""
    return (processed_count % MEMORY_CHECK_INTERVAL == 0 and 
            _get_memory_usage_mb() > memory_threshold_mb)


class StreamingConcurrencyController:
    """æµå¼å¹¶å‘æ§åˆ¶å™¨ - å¿«é€Ÿå“åº”çš„åŠ¨æ€è°ƒèŠ‚ç­–ç•¥"""
    
    def __init__(self, rpm_limit: int = DEFAULT_RPM_LIMIT):
        self.rpm_limit = rpm_limit
        self.safe_rps = (rpm_limit * 0.8) / 60  # å®‰å…¨è¯·æ±‚ç‡ (80%è£•é‡)
        self.current_concurrency = 3  # åˆå§‹å¹¶å‘æ•°
        self.success_count = 0
        self.failure_count = 0
        self.rate_limit_failures = 0  # ä¸“é—¨è®°å½•é™åˆ¶é”™è¯¯
        self.last_adjust_time = time.time()
        self.adjust_interval = 3.0  # 3ç§’å¿«é€Ÿè°ƒæ•´é—´éš”
        self.recent_requests = []  # è®°å½•æœ€è¿‘è¯·æ±‚çš„æ—¶é—´æˆ³ï¼Œç”¨äºRPSè®¡ç®—
        self.window_size = 30.0  # 30ç§’æ»‘åŠ¨çª—å£
        
        logger.info(f"[æµå¼å¹¶å‘æ§åˆ¶] å¿«é€Ÿè°ƒèŠ‚æ¨¡å¼: RPMé™åˆ¶={rpm_limit}, å®‰å…¨RPS={self.safe_rps:.1f}, åˆå§‹å¹¶å‘={self.current_concurrency}")
    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        now = time.time()
        self.success_count += 1
        self.recent_requests.append(now)
        self._cleanup_old_requests(now)
        
    def record_failure(self, is_rate_limit: bool = False, is_critical: bool = False):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        now = time.time()
        self.failure_count += 1
        if is_rate_limit:
            self.rate_limit_failures += 1
            # å¦‚æœé‡åˆ°ä¸¥é‡é™åˆ¶é”™è¯¯ï¼Œç«‹å³è§¦å‘è°ƒèŠ‚
            if is_critical:
                self.last_adjust_time = now - self.adjust_interval
    
    def _cleanup_old_requests(self, now: float):
        """æ¸…ç†çª—å£å¤–çš„è¯·æ±‚è®°å½•"""
        cutoff = now - self.window_size
        self.recent_requests = [t for t in self.recent_requests if t > cutoff]
    
    def get_current_rps(self) -> float:
        """è®¡ç®—å½“å‰å®é™…RPS"""
        if len(self.recent_requests) < 2:
            return 0.0
        return len(self.recent_requests) / min(self.window_size, time.time() - self.recent_requests[0])
    
    def should_adjust_concurrency(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´å¹¶å‘æ•° - æ›´é¢‘ç¹çš„æ£€æŸ¥"""
        now = time.time()
        
        # å¿«é€Ÿæ£€æŸ¥æ¡ä»¶
        if (now - self.last_adjust_time) >= self.adjust_interval:
            return True
        
        # å¦‚æœé‡åˆ°é™åˆ¶é”™è¯¯ï¼Œç«‹å³è°ƒèŠ‚
        if self.rate_limit_failures > 0:
            return True
            
        return False
    
    def calculate_new_concurrency(self) -> int:
        """åŸºäºå¯å‘å¼è§„åˆ™è®¡ç®—æ–°çš„å¹¶å‘æ•° - æ›´æ¿€è¿›çš„è°ƒèŠ‚ç­–ç•¥"""
        total_requests = self.success_count + self.failure_count
        
        # å¯¹äºå°‘é‡æ ·æœ¬ä¹Ÿè¿›è¡Œè°ƒèŠ‚
        if total_requests < 3:
            return self.current_concurrency
            
        success_rate = self.success_count / total_requests
        rate_limit_rate = self.rate_limit_failures / total_requests
        current_rps = self.get_current_rps()
        
        new_concurrency = self.current_concurrency
        
        # è§„åˆ™1: é‡åˆ°ä»»ä½•é™åˆ¶é”™è¯¯éƒ½ç«‹å³å‡å°‘å¹¶å‘
        if self.rate_limit_failures > 0:
            reduction = max(1, self.rate_limit_failures)  # æŒ‰å¤±è´¥æ¬¡æ•°å‡å°‘
            new_concurrency = max(1, self.current_concurrency - reduction)
            logger.warning(f"[æµå¼å¹¶å‘æ§åˆ¶] APIé™åˆ¶é”™è¯¯{self.rate_limit_failures}æ¬¡ï¼Œç«‹å³å‡å°‘å¹¶å‘: {self.current_concurrency} â†’ {new_concurrency}")
        
        # è§„åˆ™2: æˆåŠŸç‡å¾ˆé«˜ä¸”RPSæœªè¾¾ä¸Šé™ï¼Œæ¿€è¿›å¢åŠ å¹¶å‘
        elif success_rate > 0.9 and current_rps < self.safe_rps * 0.8 and self.current_concurrency < 8:
            # æ ¹æ®æˆåŠŸç‡å†³å®šå¢åŠ å¹…åº¦
            if success_rate > 0.98:
                increase = 2  # éå¸¸é«˜æˆåŠŸç‡ï¼Œå¿«é€Ÿå¢é•¿
            else:
                increase = 1
            new_concurrency = min(8, self.current_concurrency + increase)
            logger.info(f"[æµå¼å¹¶å‘æ§åˆ¶] é«˜æˆåŠŸç‡({success_rate:.1%})ä¸”RPSæœªæ»¡({current_rps:.1f}/{self.safe_rps:.1f})ï¼Œå¢åŠ å¹¶å‘: {self.current_concurrency} â†’ {new_concurrency}")
        
        # è§„åˆ™3: æˆåŠŸç‡åä½ï¼Œå‡å°‘å¹¶å‘
        elif success_rate < 0.85:
            reduction = 2 if success_rate < 0.7 else 1  # æˆåŠŸç‡å¾ˆä½æ—¶æ›´å¤§å¹…åº¦å‡å°‘
            new_concurrency = max(1, self.current_concurrency - reduction)
            logger.warning(f"[æµå¼å¹¶å‘æ§åˆ¶] æˆåŠŸç‡ä½({success_rate:.1%})ï¼Œå‡å°‘å¹¶å‘: {self.current_concurrency} â†’ {new_concurrency}")
        
        # è§„åˆ™4: RPSæ¥è¿‘ä¸Šé™ï¼Œé¢„é˜²æ€§å‡å°‘å¹¶å‘
        elif current_rps > self.safe_rps * 0.9:
            new_concurrency = max(1, self.current_concurrency - 1)
            logger.info(f"[æµå¼å¹¶å‘æ§åˆ¶] RPSæ¥è¿‘ä¸Šé™({current_rps:.1f}/{self.safe_rps:.1f})ï¼Œé¢„é˜²æ€§å‡å°‘å¹¶å‘: {self.current_concurrency} â†’ {new_concurrency}")
        
        return new_concurrency
    
    def adjust_concurrency(self) -> int:
        """è°ƒæ•´å¹¶å‘æ•°å¹¶è¿”å›æ–°å€¼ - å¿«é€Ÿé‡ç½®ç»Ÿè®¡"""
        if not self.should_adjust_concurrency():
            return self.current_concurrency
            
        old_concurrency = self.current_concurrency
        self.current_concurrency = self.calculate_new_concurrency()
        
        if self.current_concurrency != old_concurrency:
            # éƒ¨åˆ†é‡ç½®ç»Ÿè®¡è®¡æ•°å™¨ï¼Œä¿ç•™ä¸€äº›å†å²ä¿¡æ¯ç”¨äºRPSè®¡ç®—
            self.success_count = max(0, self.success_count // 2)  # ä¿ç•™ä¸€åŠå†å²
            self.failure_count = max(0, self.failure_count // 2)
            self.rate_limit_failures = 0  # é™åˆ¶é”™è¯¯ç«‹å³æ¸…é›¶
            
        self.last_adjust_time = time.time()
        return self.current_concurrency


class StreamingDocumentProcessor:
    """å†…å­˜å®‰å…¨çš„æµå¼æ–‡æ¡£å¤„ç†å™¨ - å§‹ç»ˆä¿æŒå›ºå®šæ•°é‡çš„è¯·æ±‚åœ¨æ‰§è¡Œï¼Œä¸¥æ ¼æ§åˆ¶å†…å­˜ä½¿ç”¨"""
    
    def __init__(self, vecdb: FaissVecDB, collection_name: str, rpm_limit: int = DEFAULT_RPM_LIMIT, max_memory_mb: float = DEFAULT_MAX_MEMORY_MB):
        self.vecdb = vecdb
        self.collection_name = collection_name
        self.controller = StreamingConcurrencyController(rpm_limit)
        self.document_queue = asyncio.Queue()
        self.result_queue = asyncio.Queue()
        self.active_consumers = set()
        self.total_documents = 0
        self.processed_documents = 0
        
        # å†…å­˜å®‰å…¨é…ç½®
        self.max_memory_mb = max_memory_mb
        self.initial_memory_mb = _get_memory_usage_mb()
        self.last_memory_check = time.time()
        self.memory_check_interval = 5.0  # 5ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜
        self.high_memory_threshold = max_memory_mb * 0.8  # 80%å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
        self.critical_memory_threshold = max_memory_mb * 0.95  # 95%ä½¿ç”¨ç‡ç´§æ€¥å¤„ç†
        
        # èµ„æºç®¡ç†
        self.processed_docs_cleanup_count = 0
        self.cleanup_interval = 20  # æ¯å¤„ç†20ä¸ªæ–‡æ¡£è¿›è¡Œä¸€æ¬¡èµ„æºæ¸…ç†
        
        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] åˆå§‹åŒ–: RPMé™åˆ¶={rpm_limit}, å†…å­˜é™åˆ¶={max_memory_mb}MB, "
                   f"åˆå§‹å†…å­˜={self.initial_memory_mb:.1f}MB")
        
    def _check_memory_status(self) -> tuple[bool, float, str]:
        """
        æ£€æŸ¥å†…å­˜çŠ¶æ€
        è¿”å›: (éœ€è¦é‡‡å–è¡ŒåŠ¨, å½“å‰å†…å­˜ä½¿ç”¨MB, çŠ¶æ€æè¿°)
        """
        current_memory = _get_memory_usage_mb()
        memory_increase = current_memory - self.initial_memory_mb
        
        if memory_increase > self.critical_memory_threshold:
            return True, current_memory, f"ç´§æ€¥: å†…å­˜å¢é•¿{memory_increase:.1f}MBï¼Œè¶…è¿‡ä¸´ç•Œé˜ˆå€¼"
        elif memory_increase > self.high_memory_threshold:
            return True, current_memory, f"å‘Šè­¦: å†…å­˜å¢é•¿{memory_increase:.1f}MBï¼Œæ¥è¿‘é™åˆ¶"
        else:
            return False, current_memory, f"æ­£å¸¸: å†…å­˜å¢é•¿{memory_increase:.1f}MB"
    
    async def _memory_monitor(self):
        """å†…å­˜ç›‘æ§åç¨‹ - æŒç»­ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        while True:
            try:
                await asyncio.sleep(self.memory_check_interval)
                
                needs_action, current_memory, status = self._check_memory_status()
                
                if needs_action:
                    logger.warning(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] å†…å­˜ç›‘æ§: {status}")
                    
                    # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œè§¦å‘åƒåœ¾å›æ”¶
                    if current_memory - self.initial_memory_mb > self.critical_memory_threshold:
                        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] è§¦å‘ç´§æ€¥åƒåœ¾å›æ”¶: å½“å‰{current_memory:.1f}MB")
                        gc.collect()
                        after_gc_memory = _get_memory_usage_mb()
                        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] GCå®Œæˆ: {current_memory:.1f}MB â†’ {after_gc_memory:.1f}MB "
                                  f"(é‡Šæ”¾{current_memory-after_gc_memory:.1f}MB)")
                        
                        # å¦‚æœGCåå†…å­˜ä»ç„¶å¾ˆé«˜ï¼Œå‡å°‘å¹¶å‘æ•°
                        if after_gc_memory - self.initial_memory_mb > self.high_memory_threshold:
                            old_concurrency = self.controller.current_concurrency
                            if old_concurrency > 1:
                                new_concurrency = max(1, old_concurrency - 1)
                                self.controller.current_concurrency = new_concurrency
                                await self._adjust_consumer_count(new_concurrency)
                                logger.warning(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] å†…å­˜å‹åŠ›è¿‡å¤§ï¼Œé™ä½å¹¶å‘: {old_concurrency} â†’ {new_concurrency}")
                else:
                    # å®šæœŸæŠ¥å‘Šå†…å­˜çŠ¶æ€
                    if time.time() - self.last_memory_check > 30:  # 30ç§’æŠ¥å‘Šä¸€æ¬¡
                        logger.debug(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] å†…å­˜çŠ¶æ€: {status}")
                        self.last_memory_check = time.time()
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] å†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(self.memory_check_interval)
    
    async def _progress_monitor(self):
        """è¿›åº¦ç›‘æ§åç¨‹ - æ£€æµ‹å¤„ç†åœæ»å’Œæ­»é”"""
        last_processed = 0
        stalled_count = 0
        
        while True:
            try:
                await asyncio.sleep(30)  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
                
                current_processed = self.processed_documents
                queue_size = self.document_queue.qsize()
                active_workers = len(self.active_consumers)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¿›åº¦
                if current_processed == last_processed and queue_size > 0:
                    stalled_count += 1
                    logger.warning(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] âš ï¸ è¿›åº¦åœæ»æ£€æµ‹: è¿ç»­{stalled_count}æ¬¡æ— è¿›å±•, "
                                 f"å·²å¤„ç†{current_processed}/{self.total_documents}, é˜Ÿåˆ—å‰©ä½™{queue_size}, "
                                 f"æ´»è·ƒworker{active_workers}")
                    
                    # è¿ç»­3æ¬¡æ— è¿›å±•ï¼Œå¯èƒ½æ­»é”
                    if stalled_count >= 3:
                        logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] ğŸ’€ ç–‘ä¼¼æ­»é”: é˜Ÿåˆ—{queue_size}ä¸ªæ–‡æ¡£ï¼Œä½†æ— workerå¤„ç†")
                        
                        # å°è¯•é‡å¯ä¸€ä¸ªworker
                        if active_workers > 0 and queue_size > 0:
                            logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] ğŸ”„ å°è¯•é‡å¯workerè§£å†³æ­»é”")
                            new_worker = asyncio.create_task(self._document_consumer(f"recovery-worker"))
                            self.active_consumers.add(new_worker)
                else:
                    # æœ‰è¿›å±•ï¼Œé‡ç½®è®¡æ•°å™¨
                    if stalled_count > 0:
                        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] âœ… è¿›åº¦æ¢å¤: {current_processed}/{self.total_documents}")
                    stalled_count = 0
                
                last_processed = current_processed
                
                # è¯¦ç»†çŠ¶æ€æŠ¥å‘Š
                progress = (current_processed / self.total_documents * 100) if self.total_documents > 0 else 0
                logger.debug(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] çŠ¶æ€æ£€æŸ¥: è¿›åº¦{progress:.1f}%, é˜Ÿåˆ—{queue_size}, worker{active_workers}")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] è¿›åº¦ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(30)
        
    async def process_documents_streaming(self, documents: List[Document]) -> Tuple[List[str], int]:
        """å†…å­˜å®‰å…¨çš„æµå¼å¤„ç†æ‰€æœ‰æ–‡æ¡£ - å¸¦è¶…æ—¶å’Œæ­»é”é˜²æŠ¤"""
        if not documents:
            return [], 0
            
        self.total_documents = len(documents)
        start_time = time.time()
        
        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] ğŸš€ å¼€å§‹å¤„ç† {self.total_documents} ä¸ªæ–‡æ¡£ï¼Œåˆå§‹å¹¶å‘æ•°: {self.controller.current_concurrency}, "
                   f"å†…å­˜é™åˆ¶: {self.max_memory_mb}MB")
        
        # 1. å°†æ‰€æœ‰æ–‡æ¡£æ”¾å…¥é˜Ÿåˆ—
        for doc in documents:
            await self.document_queue.put(doc)
        
        # 2. å¯åŠ¨åˆå§‹æ¶ˆè´¹è€…
        await self._start_consumers(self.controller.current_concurrency)
        
        # 3. å¯åŠ¨ç›‘æ§å’Œè°ƒèŠ‚å™¨
        adjuster_task = asyncio.create_task(self._concurrency_adjuster())
        memory_monitor_task = asyncio.create_task(self._memory_monitor())
        progress_monitor_task = asyncio.create_task(self._progress_monitor())
        
        try:
            # 4. ç­‰å¾…æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå¸¦è¶…æ—¶ä¿æŠ¤
            timeout_seconds = max(300, self.total_documents * 2)  # è‡³å°‘5åˆ†é’Ÿï¼Œæ¯æ–‡æ¡£æœ€å¤š2ç§’
            logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] è®¾ç½®è¶…æ—¶: {timeout_seconds}ç§’")
            
            await asyncio.wait_for(self.document_queue.join(), timeout=timeout_seconds)
            logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] âœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ")
            
        except asyncio.TimeoutError:
            remaining = self.document_queue.qsize()
            processed = self.total_documents - remaining
            logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] â° å¤„ç†è¶…æ—¶: å·²å¤„ç†{processed}/{self.total_documents}, å‰©ä½™{remaining}ä¸ªæ–‡æ¡£")
            
            # æ¸…ç©ºé˜Ÿåˆ—ï¼Œé˜²æ­¢join()ç»§ç»­é˜»å¡
            while not self.document_queue.empty():
                try:
                    self.document_queue.get_nowait()
                    self.document_queue.task_done()
                except:
                    break
        
        except Exception as e:
            logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] ğŸ’¥ å¤„ç†å¼‚å¸¸: {type(e).__name__}: {e}")
        
        # 5. æ¸…ç†ä»»åŠ¡
        adjuster_task.cancel()
        memory_monitor_task.cancel() 
        progress_monitor_task.cancel()
        await self._stop_all_consumers()
        
        # 6. æ”¶é›†ç»“æœ
        results = await self._collect_results()
        
        # 7. æœ€ç»ˆå†…å­˜æ¸…ç†
        documents.clear()
        gc.collect()
        final_memory = _get_memory_usage_mb()
        memory_delta = final_memory - self.initial_memory_mb
        
        # 8. ç»Ÿè®¡å’Œæ—¥å¿—
        end_time = time.time()
        processing_time = end_time - start_time
        success_count = len([r for r in results if r[0] == "success"])
        failed_count = len([r for r in results if r[0] == "failed"])
        
        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] âœ… å¤„ç†å®Œæˆ: æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, "
                   f"ç”¨æ—¶{processing_time:.1f}ç§’, å¹³å‡{self.total_documents/processing_time:.1f}æ–‡æ¡£/ç§’, "
                   f"å†…å­˜å˜åŒ–{memory_delta:+.1f}MB ({self.initial_memory_mb:.1f}â†’{final_memory:.1f}MB)")
        
        successful_doc_ids = [r[1] for r in results if r[0] == "success" and r[1]]
        return successful_doc_ids, failed_count
    
    async def _start_consumers(self, count: int):
        """å¯åŠ¨æŒ‡å®šæ•°é‡çš„æ¶ˆè´¹è€…"""
        for i in range(count):
            consumer = asyncio.create_task(self._document_consumer(f"worker-{i}"))
            self.active_consumers.add(consumer)
    
    async def _stop_all_consumers(self):
        """åœæ­¢æ‰€æœ‰æ¶ˆè´¹è€…"""
        for consumer in self.active_consumers:
            consumer.cancel()
        
        # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…åœæ­¢
        if self.active_consumers:
            await asyncio.gather(*self.active_consumers, return_exceptions=True)
        self.active_consumers.clear()
    
    async def _concurrency_adjuster(self):
        """åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°çš„åç¨‹ - å¿«é€Ÿå“åº”æ¨¡å¼"""
        while True:
            try:
                await asyncio.sleep(1)  # 1ç§’é«˜é¢‘æ£€æŸ¥
                
                old_concurrency = self.controller.current_concurrency
                new_concurrency = self.controller.adjust_concurrency()
                
                if new_concurrency != old_concurrency:
                    await self._adjust_consumer_count(new_concurrency)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[æµå¼æ–‡æ¡£å¤„ç†] å¹¶å‘è°ƒèŠ‚å™¨å¼‚å¸¸: {e}")
                await asyncio.sleep(1)  # å¼‚å¸¸åçŸ­æš‚ç­‰å¾…
    
    async def _adjust_consumer_count(self, target_count: int):
        """è°ƒæ•´æ¶ˆè´¹è€…æ•°é‡"""
        current_count = len(self.active_consumers)
        
        if target_count > current_count:
            # å¢åŠ æ¶ˆè´¹è€…
            for i in range(target_count - current_count):
                consumer = asyncio.create_task(self._document_consumer(f"worker-{current_count + i}"))
                self.active_consumers.add(consumer)
            logger.info(f"[æµå¼æ–‡æ¡£å¤„ç†] å¢åŠ æ¶ˆè´¹è€…: {current_count} â†’ {target_count}")
            
        elif target_count < current_count:
            # å‡å°‘æ¶ˆè´¹è€… (è®©ä¸€äº›æ¶ˆè´¹è€…è‡ªç„¶ç»“æŸ)
            consumers_to_stop = list(self.active_consumers)[target_count:]
            for consumer in consumers_to_stop:
                consumer.cancel()
                self.active_consumers.discard(consumer)
            logger.info(f"[æµå¼æ–‡æ¡£å¤„ç†] å‡å°‘æ¶ˆè´¹è€…: {current_count} â†’ {target_count}")
    
    async def _document_consumer(self, worker_id: str):
        """å†…å­˜å®‰å…¨çš„æ–‡æ¡£å¤„ç†æ¶ˆè´¹è€… - åŠæ—¶é‡Šæ”¾èµ„æºï¼Œç¡®ä¿task_doneè°ƒç”¨"""
        processed_by_worker = 0
        last_cleanup = time.time()
        
        while True:
            doc = None
            try:
                # ç›´æ¥ä»é˜Ÿåˆ—è·å–æ–‡æ¡£ï¼Œæ— è¶…æ—¶ç­‰å¾…
                doc = await self.document_queue.get()
                
                # é¢„å¤„ç†ï¼šè®°å½•æ–‡æ¡£å¤§å°ä»¥ä¾¿å†…å­˜ç›‘æ§
                doc_size_estimate = len(doc.text_content) if doc.text_content else 0
                
                try:
                    # å¤„ç†æ–‡æ¡£
                    result = await self._process_single_document_with_retry(doc, worker_id)
                    await self.result_queue.put(result)
                    
                    # ç«‹å³æ¸…ç†æ–‡æ¡£å¼•ç”¨ï¼Œç¡®ä¿å†…å­˜é‡Šæ”¾
                    doc.text_content = ""
                    doc.metadata.clear()
                    del doc  # æ˜¾å¼åˆ é™¤æ–‡æ¡£å¯¹è±¡
                    doc = None  # é˜²æ­¢finallyä¸­é‡å¤å¤„ç†
                    
                    # æ›´æ–°ç»Ÿè®¡
                    processed_by_worker += 1
                    self.processed_documents += 1
                    self.processed_docs_cleanup_count += 1
                    
                    # å®šæœŸè¿›è¡Œèµ„æºæ¸…ç†
                    if self.processed_docs_cleanup_count >= self.cleanup_interval:
                        current_time = time.time()
                        if current_time - last_cleanup > 10:  # è‡³å°‘é—´éš”10ç§’
                            gc.collect()  # è§¦å‘åƒåœ¾å›æ”¶
                            self.processed_docs_cleanup_count = 0
                            last_cleanup = current_time
                            
                            memory_status = self._check_memory_status()
                            logger.debug(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] {worker_id} å®šæœŸæ¸…ç†: å·²å¤„ç†{processed_by_worker}ä¸ªæ–‡æ¡£, {memory_status[2]}")
                    
                    # æ›´é¢‘ç¹çš„è¿›åº¦æŠ¥å‘Š
                    if self.processed_documents % 15 == 0:  # æ¯15ä¸ªæ–‡æ¡£æŠ¥å‘Šä¸€æ¬¡
                        progress = (self.processed_documents / self.total_documents) * 100
                        remaining = self.total_documents - self.processed_documents
                        current_memory = _get_memory_usage_mb()
                        memory_delta = current_memory - self.initial_memory_mb
                        
                        logger.info(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] è¿›åº¦: {self.processed_documents}/{self.total_documents} ({progress:.1f}%) "
                                  f"å‰©ä½™{remaining}, å†…å­˜å¢é•¿{memory_delta:+.1f}MB")
                
                except Exception as doc_error:
                    # æ–‡æ¡£å¤„ç†å¼‚å¸¸ï¼Œè®°å½•å¤±è´¥ç»“æœ
                    logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] {worker_id} æ–‡æ¡£å¤„ç†å¼‚å¸¸: {doc_error}")
                    error_result = ("failed", "", f"{type(doc_error).__name__}: {str(doc_error)}")
                    await self.result_queue.put(error_result)
                    
                    # æ¸…ç†å¼‚å¸¸æ–‡æ¡£
                    if doc:
                        doc.text_content = ""
                        doc.metadata.clear()
                        del doc
                        doc = None
                
                finally:
                    # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½å¿…é¡»è°ƒç”¨task_done
                    try:
                        self.document_queue.task_done()
                    except ValueError as e:
                        logger.warning(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] {worker_id} task_doneå¼‚å¸¸: {e}")
                
            except asyncio.CancelledError:
                logger.debug(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] {worker_id} æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œå·²å¤„ç† {processed_by_worker} ä¸ªæ–‡æ¡£")
                # å¦‚æœè¿˜æœ‰æ–‡æ¡£åœ¨å¤„ç†ï¼Œéœ€è¦æ ‡è®°å®Œæˆ
                if doc is not None:
                    try:
                        self.document_queue.task_done()
                    except ValueError:
                        pass
                break
            except Exception as e:
                logger.error(f"[å†…å­˜å®‰å…¨æµå¼å¤„ç†] {worker_id} æ¶ˆè´¹è€…å¼‚å¸¸: {e}")
                # ç¡®ä¿å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè°ƒç”¨task_done
                if doc is not None:
                    try:
                        self.document_queue.task_done()
                    except ValueError:
                        pass
                # çŸ­æš‚ç­‰å¾…åç»§ç»­å¤„ç†
                await asyncio.sleep(0.1)
    
    async def _process_single_document_with_retry(self, doc: Document, worker_id: str) -> Tuple[str, str, str]:
        """å¸¦æ™ºèƒ½é‡è¯•çš„å•æ–‡æ¡£å¤„ç†"""
        for attempt in range(EMBEDDING_MAX_RETRIES + 1):
            try:
                doc_id = await self.vecdb.insert(
                    content=doc.text_content,
                    metadata=doc.metadata,
                )
                
                # åŠæ—¶æ¸…ç†æ–‡æ¡£å¼•ç”¨
                doc.text_content = ""
                doc.metadata.clear()
                
                # è®°å½•æˆåŠŸ
                self.controller.record_success()
                return ("success", doc_id, "")
                
            except Exception as e:
                # æ™ºèƒ½é”™è¯¯æ£€æµ‹
                is_rate_limit, is_critical = _is_rate_limit_error(e)
                
                if is_rate_limit and attempt < EMBEDDING_MAX_RETRIES:
                    # APIé™åˆ¶é”™è¯¯ï¼Œæ™ºèƒ½é€€é¿é‡è¯•
                    if is_critical:
                        # ä¸¥é‡é™åˆ¶é”™è¯¯ï¼Œæ›´é•¿çš„é€€é¿æ—¶é—´
                        delay = EMBEDDING_RETRY_DELAY * (3 ** attempt)
                        logger.warning(f"[æµå¼æ–‡æ¡£å¤„ç†] {worker_id} ä¸¥é‡APIé™åˆ¶é”™è¯¯ï¼Œ{delay:.1f}såé‡è¯• (ç¬¬{attempt+1}æ¬¡)")
                    else:
                        # æ™®é€šé™åˆ¶é”™è¯¯ï¼ŒæŒ‡æ•°é€€é¿
                        delay = EMBEDDING_RETRY_DELAY * (2 ** attempt)
                        logger.warning(f"[æµå¼æ–‡æ¡£å¤„ç†] {worker_id} APIé™åˆ¶é”™è¯¯ï¼Œ{delay:.1f}såé‡è¯• (ç¬¬{attempt+1}æ¬¡)")
                    
                    await asyncio.sleep(delay)
                    self.controller.record_failure(is_rate_limit=True, is_critical=is_critical)
                    continue
                else:
                    # æœ€ç»ˆå¤±è´¥
                    self.controller.record_failure(is_rate_limit=is_rate_limit, is_critical=is_critical)
                    error_msg = f"{type(e).__name__}: {str(e)}"
                    
                    if is_rate_limit:
                        logger.error(f"[æµå¼æ–‡æ¡£å¤„ç†] {worker_id} APIé™åˆ¶é”™è¯¯é‡è¯•è€—å°½: {error_msg}")
                    else:
                        logger.error(f"[æµå¼æ–‡æ¡£å¤„ç†] {worker_id} æ–‡æ¡£å¤„ç†å¤±è´¥: {error_msg}")
                    
                    return ("failed", "", error_msg)
    
    async def _collect_results(self) -> List[Tuple[str, str, str]]:
        """æ”¶é›†æ‰€æœ‰å¤„ç†ç»“æœ"""
        results = []
        while not self.result_queue.empty():
            try:
                result = self.result_queue.get_nowait()
                results.append(result)
            except asyncio.QueueEmpty:
                break
        return results


class AstrBotEmbeddingProviderWrapper(EmbeddingProvider):
    """AstrBot Embedding Provider åŒ…è£…ç±»"""

    def __init__(
        self,
        embedding_util: EmbeddingSolutionHelper,
        collection_name: str,
    ):
        self.embedding_util = embedding_util
        self.collection_name = collection_name

    async def get_embedding(self, text: str) -> List[float]:
        vec = await self.embedding_util.get_embedding_async(text, self.collection_name)
        if not vec:
            raise ValueError(
                "è·å–å‘é‡å¤±è´¥ï¼Œè¿”å›çš„å‘é‡ä¸ºç©ºæˆ–æ— æ•ˆã€‚è¯·æ£€æŸ¥è¾“å…¥æ–‡æœ¬å’Œé…ç½®ã€‚"
            )
        return vec

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬çš„åµŒå…¥"""
        vecs = await self.embedding_util.get_embeddings_async(
            texts, self.collection_name
        )
        if not vecs:
            raise ValueError(
                "è·å–å‘é‡å¤±è´¥ï¼Œè¿”å›çš„å‘é‡ä¸ºç©ºæˆ–æ— æ•ˆã€‚è¯·æ£€æŸ¥è¾“å…¥æ–‡æœ¬å’Œé…ç½®ã€‚"
            )
        return vecs

    def get_dim(self) -> int:
        return self.embedding_util.get_dimensions(self.collection_name)


class FaissStore(VectorDBBase):
    """
    å¯¹ AstrBot FaissVecDB çš„åŒ…è£…ç±»ï¼Œä»¥é€‚åº” KB çš„æ¥å£è§„èŒƒ
    ä½¿ç”¨ LRU Cache æŒ‰éœ€åŠ è½½å’Œç®¡ç†çŸ¥è¯†åº“é›†åˆ
    """

    def __init__(
        self,
        embedding_util: EmbeddingSolutionHelper,
        data_path: str,
        max_cache_size: int = DEFAULT_MAX_CACHE_SIZE,
        max_memory_limit_mb: float = DEFAULT_MAX_MEMORY_MB,
    ):
        super().__init__(embedding_util, data_path)
        # self.vecdbs: Dict[str, FaissVecDB] = {} # è¢« cache æ›¿ä»£

        # ---- LRU Cache ç›¸å…³ ----
        # LRU ç¼“å­˜ï¼Œå­˜å‚¨ collection_name -> FaissVecDB å®ä¾‹
        self.cache: LRUCache[str, FaissVecDB] = LRUCache(maxsize=max_cache_size)
        # è®°å½•ç£ç›˜ä¸Šæ‰€æœ‰å·²çŸ¥çš„æ–°æ ¼å¼é›†åˆåç§°ï¼ˆæ— è®ºæ˜¯å¦åŠ è½½ï¼‰
        self._all_known_collections: Set[str] = set()
        # åŠ è½½é”ï¼Œé˜²æ­¢åŒä¸€é›†åˆå¹¶å‘åŠ è½½
        self._locks: Dict[str, asyncio.Lock] = {}
        self.max_cache_size = max_cache_size
        # å†…å­˜ç®¡ç†é…ç½®
        self.max_memory_limit_mb = max_memory_limit_mb
        self.memory_batch_size = DEFAULT_MEMORY_BATCH_SIZE
        logger.info(
            f"[çŸ¥è¯†åº“-ç¼“å­˜] FaissStore LRUç¼“å­˜åˆå§‹åŒ–å®Œæˆ: æœ€å¤§ç¼“å­˜å¤§å°={max_cache_size}, å†…å­˜é™åˆ¶={max_memory_limit_mb}MB"
        )
        # ------------------------

        self._old_faiss_store: Optional[OldFaissStore] = None
        self._old_collections: Dict[str, str] = {}  # è®°å½•æ‰€æœ‰æ—§æ ¼å¼çš„é›†åˆ
        self.embedding_utils: Dict[str, AstrBotEmbeddingProviderWrapper] = {}
        os.makedirs(self.data_path, exist_ok=True)

    async def initialize(self):
        """åˆå§‹åŒ–ï¼šä»…æ‰«æç£ç›˜ï¼Œä¸åŠ è½½ä»»ä½•é›†åˆåˆ°å†…å­˜"""
        logger.info(f"[çŸ¥è¯†åº“-åˆå§‹åŒ–] å¼€å§‹æ‰«æFaisså­˜å‚¨è·¯å¾„: {self.data_path}")
        # åˆå§‹åŒ–æ—¶åªæ‰«æï¼Œä¸åŠ è½½
        await self._scan_collections_on_disk()
        logger.info(
            f"[çŸ¥è¯†åº“-åˆå§‹åŒ–] æ‰«æå®Œæˆ - æ–°æ ¼å¼é›†åˆ: {len(self._all_known_collections)}ä¸ª {list(self._all_known_collections)}, "
            f"æ—§æ ¼å¼é›†åˆ: {len(self._old_collections)}ä¸ª {list(self._old_collections.keys())}"
        )

    def _get_collection_meta(self, collection_name: str) -> Tuple[str, str, str, str]:
        """å·¥å…·å‡½æ•°ï¼šæ ¹æ®é›†åˆåè·å–çœŸå®åç§°, file_id å’Œè·¯å¾„"""
        true_coll_name = (
            self.embedding_util.user_prefs_handler.get_collection_name_by_file_id(
                collection_name
            )
        )
        # æ£€æŸ¥å…ƒæ•°æ®
        collection_md = (
            self.embedding_util.user_prefs_handler.user_collection_preferences.get(
                "collection_metadata", {}
            ).get(collection_name, {})
        )

        if true_coll_name:
            # collection_name is actually a file_id
            file_id = collection_name
            final_collection_name = true_coll_name
        elif collection_md:
            # collection_name is a true name, get file_id from metadata
            file_id = collection_md.get("file_id", collection_name)
            final_collection_name = collection_name
        else:
            # fallback
            file_id = collection_name
            final_collection_name = collection_name

        index_path = os.path.join(self.data_path, f"{file_id}.index")
        storage_path = os.path.join(self.data_path, f"{file_id}.db")
        _old_storage_path = os.path.join(self.data_path, f"{file_id}.docs")
        return (
            final_collection_name,
            file_id,
            index_path,
            storage_path,
            _old_storage_path,
        )

    async def _scan_collections_on_disk(self):
        """æ‰«æç£ç›˜ç›®å½•ï¼Œè¯†åˆ«æ–°æ—§é›†åˆï¼Œå¡«å…… _all_known_collections å’Œ _old_collections"""
        self._all_known_collections.clear()
        self._old_collections.clear()
        if not os.path.exists(self.data_path):
            return

        scanned_file_ids = set()
        # ä¼˜å…ˆå¤„ç† .index å’Œ .db æ–‡ä»¶
        all_files = os.listdir(self.data_path)
        relevant_extensions = (".index", ".db", ".docs")

        for filename in all_files:
            if not filename.endswith(relevant_extensions):
                continue

            base, ext = os.path.splitext(filename)
            if base in scanned_file_ids:
                continue

            file_id = base
            collection_name, _, index_path, storage_path, _old_storage_path = (
                self._get_collection_meta(file_id)
            )

            is_old = False
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ—§æ ¼å¼
            if _check_pickle_file(storage_path) or os.path.exists(_old_storage_path):
                is_old = True
            # å¦‚æœ .index å’Œ .db éƒ½å­˜åœ¨ï¼Œè®¤ä¸ºæ˜¯æ–°æ ¼å¼ (é™¤é .db æ˜¯pickle æˆ–å­˜åœ¨ .docs)
            elif os.path.exists(index_path) and os.path.exists(storage_path):
                is_old = False
            # å¦‚æœåªæœ‰ .docsï¼Œè®¤ä¸ºæ˜¯æ—§æ ¼å¼
            elif ext == ".docs" and not os.path.exists(index_path):
                is_old = True
            else:
                # å…¶ä»–æƒ…å†µï¼Œä¾‹å¦‚åªæœ‰ .index æˆ–åªæœ‰ .db (épickle)ï¼Œæš‚æ—¶è·³è¿‡æˆ–è®¤ä¸ºæ˜¯æ–°æ ¼å¼ä¸å®Œæ•´
                # ä¸ºç®€å•èµ·è§ï¼Œå¦‚æœå­˜åœ¨ index å’Œ db ä¹‹ä¸€ä¸”éæ—§æ ¼å¼ï¼Œå°±è®¤ä¸ºæ˜¯æ–°æ ¼å¼
                if ext in (".index", ".db"):
                    is_old = False
                else:
                    continue  # å¿½ç•¥ä¸æ˜ç¡®çš„æ–‡ä»¶

            scanned_file_ids.add(file_id)
            if is_old:
                self._old_collections[collection_name] = collection_name
                logger.debug(f"[çŸ¥è¯†åº“-æ‰«æ] å‘ç°æ—§æ ¼å¼é›†åˆ: {collection_name} (file_id: {file_id})")
            else:
                self._all_known_collections.add(collection_name)
                logger.debug(f"[çŸ¥è¯†åº“-æ‰«æ] å‘ç°æ–°æ ¼å¼é›†åˆ: {collection_name} (file_id: {file_id})")

        # å¦‚æœå‘ç°äº†æ—§é›†åˆï¼Œåˆå§‹åŒ–æ—§å­˜å‚¨å®ä¾‹
        if self._old_collections and not self._old_faiss_store:
            logger.info(f"[çŸ¥è¯†åº“-æ‰«æ] æ£€æµ‹åˆ°æ—§æ ¼å¼é›†åˆï¼Œåˆå§‹åŒ–OldFaissStoreå¤„ç†å™¨...")
            self._old_faiss_store = OldFaissStore(self.embedding_util, self.data_path)
            await self._old_faiss_store.initialize()

    async def _perform_load(
        self, collection_name: str, index_path: str, storage_path: str
    ) -> FaissVecDB:
        """æ‰§è¡Œå®é™…çš„åŠ è½½/åˆ›å»º FaissVecDB é€»è¾‘ï¼Œä¸æ¶‰åŠç¼“å­˜å’Œé”"""
        logger.info(f"[çŸ¥è¯†åº“-åŠ è½½] å¼€å§‹åŠ è½½/åˆ›å»ºFaissé›†åˆå®ä¾‹: '{collection_name}'")
        self.embedding_utils[collection_name] = AstrBotEmbeddingProviderWrapper(
            embedding_util=self.embedding_util,
            collection_name=collection_name,
        )
        params = {
            "doc_store_path": storage_path,
            "index_store_path": index_path,
            "embedding_provider": self.embedding_utils[collection_name],
        }
        rerank_prov = self.embedding_util.get_rerank_provider(collection_name)
        if rerank_prov:
            params["rerank_provider"] = rerank_prov
        vecdb = FaissVecDB(**params)
        await vecdb.initialize()
        logger.info(f"[çŸ¥è¯†åº“-åŠ è½½] Faissé›†åˆå®ä¾‹ '{collection_name}' åŠ è½½/åˆ›å»ºå®Œæˆ")
        return vecdb

    async def _evict_lru_if_needed(self):
        """å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ™ç§»é™¤å¹¶å…³é—­æœ€å°‘ä½¿ç”¨çš„é›†åˆ"""
        evicted_count = 0
        while len(self.cache) >= self.max_cache_size and self.max_cache_size > 0:
            try:
                lru_key, lru_vecdb = self.cache.popitem()
                logger.info(
                    f"[çŸ¥è¯†åº“-ç¼“å­˜] ç¼“å­˜å·²æ»¡(max={self.max_cache_size})ï¼Œç§»å‡ºæœ€å°‘ä½¿ç”¨çš„é›†åˆ: '{lru_key}'"
                )
                self.embedding_utils.pop(lru_key, None)
                self._locks.pop(lru_key, None)  # æ¸…ç†é”
                try:
                    await lru_vecdb.close()
                    logger.info(f"[çŸ¥è¯†åº“-ç¼“å­˜] æˆåŠŸå…³é—­è¢«ç§»å‡ºçš„é›†åˆ: '{lru_key}'")
                    evicted_count += 1
                except Exception as close_e:
                    logger.error(f"[çŸ¥è¯†åº“-ç¼“å­˜] å…³é—­è¢«ç§»å‡ºçš„é›†åˆ '{lru_key}' æ—¶å‘ç”Ÿé”™è¯¯: {close_e}")
            except KeyError:
                # ç¼“å­˜ä¸ºç©º
                break
            except Exception as e:
                logger.error(f"[çŸ¥è¯†åº“-ç¼“å­˜] ç¼“å­˜ç§»å‡ºè¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        # å¦‚æœæœ‰ç§»å‡ºæ“ä½œï¼Œè§¦å‘åƒåœ¾å›æ”¶
        if evicted_count > 0:
            gc.collect()
            logger.debug(f"å·²ç§»å‡º {evicted_count} ä¸ªé›†åˆï¼Œè§¦å‘åƒåœ¾å›æ”¶")

    async def _unload_collection(self, collection_name: str):
        """ä»ç¼“å­˜ä¸­å¸è½½å¹¶å…³é—­ä¸€ä¸ªæŒ‡å®šçš„é›†åˆ"""
        vecdb_to_close = self.cache.pop(collection_name, None)
        self.embedding_utils.pop(collection_name, None)
        self._locks.pop(collection_name, None)  # æ¸…ç†é”
        if vecdb_to_close:
            logger.info(f"ä»ç¼“å­˜ä¸­å¸è½½å¹¶å…³é—­é›†åˆ: '{collection_name}'")
            try:
                await vecdb_to_close.close()
            except Exception as e:
                logger.error(f"å…³é—­é›†åˆ '{collection_name}' æ—¶å‡ºé”™: {e}")

    async def _get_or_load_vecdb(
        self, collection_name: str, for_create: bool = False
    ) -> Optional[FaissVecDB]:
        """
        æ ¸å¿ƒå‡½æ•°ï¼šä»ç¼“å­˜è·å–æˆ–æŒ‰éœ€åŠ è½½é›†åˆ
        1. æ£€æŸ¥ç¼“å­˜
        2. ç¼“å­˜æœªå‘½ä¸­åˆ™åŠ é”
        3. é”å†…å†æ¬¡æ£€æŸ¥ç¼“å­˜ï¼ˆDouble-Check Lockingï¼‰
        4. æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»å‡º LRU
        5. åŠ è½½é›†åˆ
        6. æ”¾å…¥ç¼“å­˜
        """
        # 1. æ—§é›†åˆæˆ–å·²åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥è¿”å›
        if collection_name in self._old_collections:
            return None
        if collection_name in self.cache:
            # è®¿é—®å³æ›´æ–°å…¶åœ¨ LRU ä¸­çš„ä½ç½®
            return self.cache[collection_name]

        # 2. è·å–æˆ–åˆ›å»ºé’ˆå¯¹æ­¤é›†åˆçš„é”
        lock = self._locks.setdefault(collection_name, asyncio.Lock())

        async with lock:
            # 3. é”å†…å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨ç­‰å¾…é”æœŸé—´å…¶ä»–åç¨‹å·²åŠ è½½
            if collection_name in self.cache:
                return self.cache[collection_name]

            logger.info(f"[çŸ¥è¯†åº“-åŠ è½½] ç¼“å­˜æœªå‘½ä¸­ï¼Œå‡†å¤‡åŠ è½½é›†åˆ: '{collection_name}'")

            _, _, index_path, storage_path, _ = self._get_collection_meta(
                collection_name
            )

            # å¦‚æœä¸æ˜¯åˆ›å»ºæ“ä½œï¼Œä¸”æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™ä¸åŠ è½½
            if not for_create and not (
                os.path.exists(index_path) and os.path.exists(storage_path)
            ):
                logger.warning(f"[çŸ¥è¯†åº“-åŠ è½½] è­¦å‘Š: é›†åˆ '{collection_name}' çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åŠ è½½ã€‚ç´¢å¼•æ–‡ä»¶: {index_path}, å­˜å‚¨æ–‡ä»¶: {storage_path}")
                # self._locks.pop(collection_name, None) # åŠ è½½å¤±è´¥ï¼Œæ¸…ç†é”
                return None

            # 4. åŠ è½½å‰æ£€æŸ¥å¹¶æ‰§è¡Œç§»å‡ºæ“ä½œ
            await self._evict_lru_if_needed()

            # 5. æ‰§è¡ŒåŠ è½½
            try:
                vecdb = await self._perform_load(
                    collection_name, index_path, storage_path
                )
                # 6. æ”¾å…¥ç¼“å­˜
                self.cache[collection_name] = vecdb
                self._all_known_collections.add(collection_name)  # ç¡®ä¿å·²è®°å½•
                logger.info(
                    f"[çŸ¥è¯†åº“-åŠ è½½] é›†åˆ '{collection_name}' å·²åŠ è½½å¹¶æ”¾å…¥ç¼“å­˜ã€‚å½“å‰ç¼“å­˜å¤§å°: {len(self.cache)}/{self.max_cache_size}"
                )
                return vecdb
            except Exception as e:
                logger.error(f"[çŸ¥è¯†åº“-åŠ è½½] åŠ è½½çŸ¥è¯†åº“é›†åˆ(FAISS) '{collection_name}' æ—¶å‡ºé”™: {type(e).__name__} - {str(e)}")
                # æ¸…ç†å¯èƒ½æ®‹ç•™çš„çŠ¶æ€
                self.cache.pop(collection_name, None)
                self.embedding_utils.pop(collection_name, None)
                # self._locks.pop(collection_name, None) # åŠ è½½å¤±è´¥ï¼Œæ¸…ç†é”
                return None
        # é”è‡ªåŠ¨é‡Šæ”¾

    # async def _load_collection(self, collection_name: str): # åºŸå¼ƒ
    # async def _load_all_collections(self): # åºŸå¼ƒï¼Œç”± _scan_collections_on_disk æ›¿ä»£æ‰«æåŠŸèƒ½

    async def create_collection(self, collection_name: str):
        """åˆ›å»ºå¹¶åŠ è½½ä¸€ä¸ªæ–°é›†åˆåˆ°ç¼“å­˜"""
        if await self.collection_exists(collection_name):
            # å¦‚æœå·²å­˜åœ¨ï¼ˆåœ¨ç£ç›˜æˆ–æ—§å­˜å‚¨ä¸­ï¼‰ï¼Œå°è¯•åŠ è½½åˆ°ç¼“å­˜ï¼ˆå¦‚æœè¿˜ä¸åœ¨ï¼‰
            logger.info(f"[çŸ¥è¯†åº“-åˆ›å»º] Faissé›†åˆ '{collection_name}' å·²å­˜åœ¨ï¼Œå°è¯•åŠ è½½åˆ°ç¼“å­˜")
            await self._get_or_load_vecdb(collection_name)
            return

        logger.info(f"[çŸ¥è¯†åº“-åˆ›å»º] å¼€å§‹åˆ›å»ºæ–°Faissé›†åˆ '{collection_name}'")
        # ä¿å­˜åå¥½è®¾ç½®
        await self.embedding_util.user_prefs_handler.save_user_preferences()

        # ä½¿ç”¨ _get_or_load_vecdb è¿›è¡Œåˆ›å»ºï¼Œå®ƒä¼šå¤„ç†é”ã€ç¼“å­˜ç§»å‡ºå’ŒåŠ è½½
        # è®¾ç½® for_create=True ä½¿å¾—å³ä½¿æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿä¼šç»§ç»­ _perform_load
        vecdb = await self._get_or_load_vecdb(collection_name, for_create=True)

        if vecdb:
            # æ–°åˆ›å»ºçš„é›†åˆéœ€è¦æ˜¾å¼ä¿å­˜ä¸€ä¸‹ç´¢å¼•æ–‡ä»¶
            await vecdb.embedding_storage.save_index()
            # _get_or_load_vecdb å·²ç»å°†å…¶åŠ å…¥ _all_known_collections
            logger.info(f"[çŸ¥è¯†åº“-åˆ›å»º] Faissé›†åˆ '{collection_name}' åˆ›å»ºæˆåŠŸå¹¶å·²åŠ è½½åˆ°ç¼“å­˜")
        else:
            logger.error(f"[çŸ¥è¯†åº“-åˆ›å»º] Faissé›†åˆ '{collection_name}' åˆ›å»ºæˆ–åŠ è½½å¤±è´¥")

    async def collection_exists(self, collection_name: str) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨äºç£ç›˜ï¼ˆæ–°æ ¼å¼ï¼‰æˆ–æ—§å­˜å‚¨ä¸­"""
        # æ£€æŸ¥å·²çŸ¥çš„ï¼ˆæ‰«æåˆ°çš„æˆ–åˆ›å»ºçš„ï¼‰æ–°æ ¼å¼é›†åˆï¼Œä»¥åŠæ—§æ ¼å¼é›†åˆ
        return (
            collection_name in self._all_known_collections
            or collection_name in self._old_collections
        )

    async def _process_documents_batch(
        self,
        documents: List[Document],
        collection_name: str,
        vecdb: FaissVecDB,
    ) -> Tuple[List[str], int]:
        """
        é¡ºåºå¤„ç†ä¸€æ‰¹æ–‡æ¡£ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ˆä¿ç•™ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        è¿”å›: (æˆåŠŸæ·»åŠ çš„æ–‡æ¡£IDåˆ—è¡¨, å¤±è´¥æ•°é‡)
        """
        if not vecdb:
            logger.error(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] è‡´å‘½é”™è¯¯: é›†åˆ '{collection_name}' çš„ vecdb å®ä¾‹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†æ–‡æ¡£")
            return [], len(documents)

        doc_ids = []
        failed_count = 0
        batch_size = len(documents)
        
        logger.debug(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] å¼€å§‹é€ä¸ªå¤„ç† {batch_size} ä¸ªæ–‡æ¡£ï¼Œé›†åˆ: '{collection_name}'")
        
        for i, doc in enumerate(documents):
            try:
                # è·å–æ–‡æ¡£é¢„è§ˆç”¨äºæ—¥å¿—
                doc_preview = doc.text_content[:50].replace("\n", " ") if doc.text_content else "[ç©ºæ–‡æ¡£]"
                
                doc_id = await vecdb.insert(
                    content=doc.text_content,
                    metadata=doc.metadata,
                )
                doc_ids.append(doc_id)
                
                # è¯¦ç»†çš„è¿›åº¦æ—¥å¿—ï¼ˆæ¯10ä¸ªæ–‡æ¡£è®°å½•ä¸€æ¬¡ï¼‰
                if (i + 1) % 10 == 0 or (i + 1) == batch_size:
                    logger.debug(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] è¿›åº¦: {i+1}/{batch_size} ä¸ªæ–‡æ¡£å·²å¤„ç†ï¼Œæœ€æ–°: '{doc_preview}...'")
                
                # åŠæ—¶æ¸…ç†æ–‡æ¡£å¼•ç”¨ä»¥é‡Šæ”¾å†…å­˜
                doc.text_content = ""
                doc.metadata.clear()
                
            except Exception as e:
                failed_count += 1
                excerpt = doc.text_content[:50].replace("\n", " ") if doc.text_content else "[ç©ºæ–‡æ¡£]"
                logger.error(
                    f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] æ–‡æ¡£å¤„ç†å¤±è´¥: ç¬¬{i+1}/{batch_size}ä¸ªæ–‡æ¡£ '{excerpt}...' æ·»åŠ åˆ°é›†åˆ '{collection_name}' å¤±è´¥ï¼Œ"
                    f"é”™è¯¯ç±»å‹: {type(e).__name__}ï¼Œé”™è¯¯è¯¦æƒ…: {str(e)}"
                )
            
            # æ¯å¤„ç†ä¸€å®šæ•°é‡çš„æ–‡æ¡£åè§¦å‘åƒåœ¾å›æ”¶
            if (i + 1) % 20 == 0:
                gc.collect()
                logger.debug(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] å·²å¤„ç† {i+1} ä¸ªæ–‡æ¡£ï¼Œæ‰§è¡Œå†…å­˜åƒåœ¾å›æ”¶")
                
        # æ¸…ç©ºæ•´ä¸ªæ‰¹æ¬¡çš„æ–‡æ¡£åˆ—è¡¨
        documents.clear()
        
        if failed_count == 0:
            logger.debug(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆ: é›†åˆ '{collection_name}' æˆåŠŸå¤„ç† {len(doc_ids)} ä¸ªæ–‡æ¡£")
        else:
            logger.warning(f"[çŸ¥è¯†åº“-æ‰¹æ¬¡å¤„ç†] âš ï¸ æ‰¹æ¬¡å¤„ç†å®Œæˆ: é›†åˆ '{collection_name}' æˆåŠŸ {len(doc_ids)} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªæ–‡æ¡£")
            
        return doc_ids, failed_count

    async def _process_documents_efficiently(
        self,
        documents: List[Document],
        collection_name: str,
        vecdb: FaissVecDB,
        use_parallel: bool = True
    ) -> Tuple[List[str], int]:
        """
        é«˜æ•ˆçš„æ–‡æ¡£æ‰¹æ¬¡å¤„ç† - åŸºäºç³»ç»Ÿå†…å­˜åŠ¨æ€è°ƒèŠ‚å¹¶å‘æ•°
        ç­–ç•¥ï¼šæ™ºèƒ½å¹¶å‘è°ƒèŠ‚ + å®æ—¶å†…å­˜ç›‘æ§ + åŠ¨æ€åƒåœ¾å›æ”¶
        """
        if not vecdb:
            logger.error(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] è‡´å‘½é”™è¯¯: é›†åˆ '{collection_name}' çš„ vecdb å®ä¾‹ä¸ºç©º")
            return [], len(documents)

        total_docs = len(documents)
        if total_docs == 0:
            return [], 0

        # åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
        optimal_concurrent, effective_memory_limit, decision_reason = _calculate_optimal_concurrency(self.max_memory_limit_mb)
        
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        initial_memory = _get_memory_usage_mb()
        total_memory, available_memory, usage_percent = _get_system_memory_info()
        
        logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] æ™ºèƒ½å¹¶å‘åˆ†æ: æ–‡æ¡£æ•°={total_docs}, å¹¶å‘ä»»åŠ¡æ•°={optimal_concurrent}({decision_reason}), "
                   f"å†…å­˜é™åˆ¶={effective_memory_limit:.0f}MB, ç³»ç»Ÿå†…å­˜={available_memory:.0f}MBå¯ç”¨")

        all_doc_ids = []
        total_failed = 0

        # æ ¹æ®åˆ†æç»“æœå†³å®šå¤„ç†ç­–ç•¥
        if not use_parallel or total_docs < 30 or optimal_concurrent <= 2:
            logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] ä½¿ç”¨é¡ºåºå¤„ç†: æ–‡æ¡£æ•°={total_docs}, å¹¶å‘æ•°={optimal_concurrent}")
            return await self._process_documents_batch(documents, collection_name, vecdb)

        # æ™ºèƒ½åˆ†å—è®¡ç®—
        concurrent_tasks = optimal_concurrent
        chunk_size = max(3, total_docs // (concurrent_tasks * 3))  # åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°
        max_parallel_chunks = concurrent_tasks * 2  # æ§åˆ¶åŒæ—¶å¤„ç†çš„å—æ•°
        
        logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å¯åŠ¨æ™ºèƒ½å¹¶å‘å¤„ç†: {concurrent_tasks}ä¸ªå¹¶å‘ä»»åŠ¡, æ¯å—{chunk_size}ä¸ªæ–‡æ¡£, "
                   f"æœ€å¤§å¹¶è¡Œå—æ•°={max_parallel_chunks}")

        # åˆ›å»ºåŠ¨æ€ä¿¡å·é‡
        semaphore = asyncio.Semaphore(concurrent_tasks)
        processed_chunks = 0
        total_chunks = (total_docs + chunk_size - 1) // chunk_size

        async def process_chunk_with_monitoring(chunk_docs: List[Document], chunk_idx: int) -> Tuple[List[str], int]:
            """å¸¦å†…å­˜ç›‘æ§çš„æ–‡æ¡£å—å¤„ç† - ä½¿ç”¨ä¸»ä¸Šä¸‹æ–‡æ—¥å¿—"""
            async with semaphore:
                chunk_start_memory = _get_memory_usage_mb()
                chunk_size_actual = len(chunk_docs)
                
                # ä½¿ç”¨ä¸»åç¨‹çš„loggerä¸Šä¸‹æ–‡è®°å½•æ—¥å¿—
                logger.debug(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx}å¼€å§‹: {chunk_size_actual}ä¸ªæ–‡æ¡£, å†…å­˜{chunk_start_memory:.1f}MB")
                
                chunk_doc_ids = []
                chunk_failed = 0
                
                # æ”¶é›†å¤„ç†è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é¢‘ç¹è®°å½•æ—¥å¿—
                failed_docs = []
                
                for i, doc in enumerate(chunk_docs):
                    try:
                        doc_id = await vecdb.insert(
                            content=doc.text_content,
                            metadata=doc.metadata,
                        )
                        chunk_doc_ids.append(doc_id)
                        
                        # åŠæ—¶æ¸…ç†æ–‡æ¡£å¼•ç”¨
                        doc.text_content = ""
                        doc.metadata.clear()
                        
                    except Exception as e:
                        chunk_failed += 1
                        excerpt = doc.text_content[:30].replace("\n", " ") if doc.text_content else "[ç©º]"
                        # æ”¶é›†å¤±è´¥ä¿¡æ¯ï¼Œç¨åç»Ÿä¸€è®°å½•
                        failed_docs.append((i+1, excerpt, type(e).__name__, str(e)))
                
                # æ¸…ç†chunkæ–‡æ¡£åˆ—è¡¨
                chunk_docs.clear()
                
                chunk_end_memory = _get_memory_usage_mb()
                memory_delta = chunk_end_memory - chunk_start_memory
                
                # ç»Ÿä¸€è®°å½•å—å¤„ç†ç»“æœï¼ˆåœ¨ä¸»åç¨‹ä¸Šä¸‹æ–‡ä¸­ï¼‰
                if chunk_failed == 0:
                    logger.debug(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx}å®Œæˆ: æˆåŠŸ{len(chunk_doc_ids)}, "
                               f"å†…å­˜å˜åŒ–{memory_delta:+.1f}MB ({chunk_start_memory:.1f}â†’{chunk_end_memory:.1f}MB)")
                else:
                    logger.warning(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx}å®Œæˆ: æˆåŠŸ{len(chunk_doc_ids)}, å¤±è´¥{chunk_failed}, "
                                 f"å†…å­˜å˜åŒ–{memory_delta:+.1f}MB ({chunk_start_memory:.1f}â†’{chunk_end_memory:.1f}MB)")
                    # è®°å½•å‰å‡ ä¸ªå¤±è´¥çš„æ–‡æ¡£è¯¦æƒ…
                    for i, (doc_idx, excerpt, error_type, error_msg) in enumerate(failed_docs[:3]):
                        logger.error(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx}æ–‡æ¡£{doc_idx}å¤±è´¥: '{excerpt}' - {error_type}: {error_msg}")
                    if len(failed_docs) > 3:
                        logger.warning(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx}è¿˜æœ‰{len(failed_docs)-3}ä¸ªæ–‡æ¡£å¤±è´¥ï¼ˆå·²çœç•¥è¯¦æƒ…ï¼‰")
                
                return chunk_doc_ids, chunk_failed

        # åˆ†æ‰¹å¤„ç†ä»»åŠ¡ï¼Œé¿å…åˆ›å»ºè¿‡å¤šå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for i in range(0, total_docs, chunk_size):
            chunk_end = min(i + chunk_size, total_docs)
            chunk_docs = documents[i:chunk_end]
            chunk_idx = i // chunk_size + 1
            
            task = asyncio.create_task(process_chunk_with_monitoring(chunk_docs, chunk_idx))
            tasks.append(task)
            
            # åˆ†æ‰¹æ‰§è¡Œï¼Œé¿å…å†…å­˜å³°å€¼è¿‡é«˜
            if len(tasks) >= max_parallel_chunks or chunk_idx == total_chunks:
                logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] æ‰§è¡Œæ‰¹æ¬¡: {len(tasks)}ä¸ªå¹¶å‘ä»»åŠ¡ (å—{chunk_idx-len(tasks)+1}-{chunk_idx})")
                
                # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # åœ¨ä¸»åç¨‹ä¸Šä¸‹æ–‡ä¸­å¤„ç†æ‰€æœ‰ç»“æœå’Œæ—¥å¿—
                successful_chunks = 0
                failed_chunks = 0
                
                for j, result in enumerate(results):
                    chunk_idx_actual = (i // chunk_size) - len(tasks) + j + 2
                    
                    if isinstance(result, Exception):
                        failed_chunks += 1
                        error_type = type(result).__name__
                        error_msg = str(result)
                        logger.error(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx_actual}å¼‚æ­¥ä»»åŠ¡å¤±è´¥: {error_type} - {error_msg}")
                        total_failed += chunk_size  # ä¼°ç®—å¤±è´¥æ•°é‡
                    else:
                        successful_chunks += 1
                        doc_ids, failed = result
                        all_doc_ids.extend(doc_ids)
                        total_failed += failed
                        
                        # å¦‚æœè¯¥å—æœ‰å¤±è´¥æ–‡æ¡£ï¼Œåœ¨ä¸»åç¨‹ä¸­è®°å½•æ±‡æ€»ä¿¡æ¯
                        if failed > 0:
                            success_in_chunk = len(doc_ids)
                            logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å—{chunk_idx_actual}æ±‡æ€»: æˆåŠŸ{success_in_chunk}, å¤±è´¥{failed}")
                
                # è®°å½•æ‰¹æ¬¡æ‰§è¡Œæ±‡æ€»
                if failed_chunks > 0:
                    logger.warning(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] æ‰¹æ¬¡æ‰§è¡Œå®Œæˆ: æˆåŠŸå—{successful_chunks}, å¼‚å¸¸å—{failed_chunks}")
                else:
                    logger.debug(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] æ‰¹æ¬¡æ‰§è¡Œå®Œæˆ: {successful_chunks}ä¸ªå—å…¨éƒ¨æˆåŠŸ")
                
                processed_chunks += len(tasks)
                tasks.clear()
                
                # æ‰¹æ¬¡å®Œæˆåçš„å†…å­˜ç®¡ç†
                current_memory = _get_memory_usage_mb()
                memory_increase = current_memory - initial_memory
                
                if memory_increase > effective_memory_limit * 0.3:  # è¶…è¿‡é™åˆ¶çš„30%
                    logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] å†…å­˜å¢é•¿{memory_increase:.1f}MBï¼Œè§¦å‘åƒåœ¾å›æ”¶ (å½“å‰{current_memory:.1f}MB)")
                    gc.collect()
                    after_gc_memory = _get_memory_usage_mb()
                    logger.debug(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] GCåå†…å­˜: {after_gc_memory:.1f}MB (é‡Šæ”¾{current_memory-after_gc_memory:.1f}MB)")
                
                # è¿›åº¦æŠ¥å‘Š
                progress_percent = (processed_chunks / total_chunks) * 100
                logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] è¿›åº¦: {processed_chunks}/{total_chunks}å— ({progress_percent:.1f}%), "
                           f"å·²å¤„ç†{len(all_doc_ids)}ä¸ªæ–‡æ¡£, å½“å‰å†…å­˜{current_memory:.1f}MB")

        # æ¸…ç†åŸå§‹æ–‡æ¡£åˆ—è¡¨
        documents.clear()
        
        # æœ€ç»ˆç»Ÿè®¡å’Œæ¸…ç†
        gc.collect()
        final_memory = _get_memory_usage_mb()
        memory_delta_total = final_memory - initial_memory
        
        success_count = len(all_doc_ids)
        success_rate = (success_count / total_docs * 100) if total_docs > 0 else 0
        
        logger.info(f"[çŸ¥è¯†åº“-é«˜æ•ˆå¤„ç†] âœ… æ™ºèƒ½å¹¶å‘å¤„ç†å®Œæˆ: æˆåŠŸ{success_count}/{total_docs} ({success_rate:.1f}%), "
                   f"å¤±è´¥{total_failed}, å¹¶å‘æ•°{concurrent_tasks}, å†…å­˜å˜åŒ–{memory_delta_total:+.1f}MB "
                   f"({initial_memory:.1f}â†’{final_memory:.1f}MB)")
        
    async def _process_documents_streaming_v2(
        self, 
        documents: List[Document], 
        collection_name: str, 
        vecdb: FaissVecDB,
        rpm_limit: int = DEFAULT_RPM_LIMIT,
        max_memory_mb: float = DEFAULT_MAX_MEMORY_MB
    ) -> Tuple[List[str], int]:
        """
        æ–°ç‰ˆå†…å­˜å®‰å…¨æµå¼æ–‡æ¡£å¤„ç† - çœŸæ­£çš„æµå¼å¤„ç†ç­–ç•¥
        ç‰¹æ€§ï¼šå§‹ç»ˆä¿æŒNä¸ªè¯·æ±‚å¹¶è¡Œæ‰§è¡Œï¼ŒåŠ¨æ€è°ƒèŠ‚ï¼Œå¿«é€Ÿå“åº”ï¼Œä¸¥æ ¼å†…å­˜æ§åˆ¶
        """
        if not vecdb:
            logger.error(f"[çŸ¥è¯†åº“-æµå¼å¤„ç†v2] è‡´å‘½é”™è¯¯: é›†åˆ '{collection_name}' çš„ vecdb å®ä¾‹ä¸ºç©º")
            return [], len(documents)
        
        total_docs = len(documents)
        if total_docs == 0:
            return [], 0
            
        # åˆ›å»ºå†…å­˜å®‰å…¨çš„æµå¼å¤„ç†å™¨
        processor = StreamingDocumentProcessor(vecdb, collection_name, rpm_limit, max_memory_mb)
        
        logger.info(f"[çŸ¥è¯†åº“-æµå¼å¤„ç†v2] ğŸš€ å¯åŠ¨å†…å­˜å®‰å…¨æµå¼å¤„ç†: {total_docs}ä¸ªæ–‡æ¡£, RPMé™åˆ¶={rpm_limit}, å†…å­˜é™åˆ¶={max_memory_mb}MB")
        
        try:
            # æ‰§è¡Œæµå¼å¤„ç†
            successful_doc_ids, failed_count = await processor.process_documents_streaming(documents)
            
            success_count = len(successful_doc_ids)
            success_rate = (success_count / total_docs * 100) if total_docs > 0 else 0
            
            logger.info(f"[çŸ¥è¯†åº“-æµå¼å¤„ç†v2] âœ… æµå¼å¤„ç†å®Œæˆ: æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, "
                       f"æˆåŠŸç‡{success_rate:.1f}%")
            
            return successful_doc_ids, failed_count
            
        except Exception as e:
            logger.error(f"[çŸ¥è¯†åº“-æµå¼å¤„ç†v2] ğŸ’¥ æµå¼å¤„ç†å¼‚å¸¸: {type(e).__name__}: {e}")
            return [], total_docs

    async def add_documents(
        self, collection_name: str, documents: List[Document]
    ) -> List[str]:
        """
        å‘æŒ‡å®šé›†åˆä¸­æ·»åŠ æ–‡æ¡£ï¼Œä½¿ç”¨é¡ºåºå¤„ç†ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        """
        # å¤„ç†æ—§é›†åˆ
        if collection_name in self._old_collections:
            if self._old_faiss_store:
                logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] æ£€æµ‹åˆ°æ—§æ ¼å¼é›†åˆ '{collection_name}'ï¼Œä½¿ç”¨æ—§å­˜å‚¨å¼•æ“å¤„ç†")
                return await self._old_faiss_store.add_documents(
                    collection_name, documents
                )
            else:
                logger.error(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] è‡´å‘½é”™è¯¯: æ—§é›†åˆ '{collection_name}' å­˜åœ¨ä½† OldFaissStore æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ’ä»¶é…ç½®")
                return []

        # æ£€æŸ¥æˆ–åˆ›å»ºé›†åˆ
        if not await self.collection_exists(collection_name):
            logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] ç›®æ ‡é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œå¼€å§‹è‡ªåŠ¨åˆ›å»ºæ–°é›†åˆ")
            await self.create_collection(collection_name)
        else:
            logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] ç›®æ ‡é›†åˆ '{collection_name}' å·²å­˜åœ¨ï¼Œå‡†å¤‡æ·»åŠ æ–‡æ¡£")

        # è·å–é›†åˆå®ä¾‹
        vecdb = await self._get_or_load_vecdb(collection_name)
        if not vecdb:
            logger.error(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] ä¸¥é‡é”™è¯¯: æ— æ³•è·å–æˆ–åŠ è½½é›†åˆ '{collection_name}' çš„å‘é‡æ•°æ®åº“å®ä¾‹ï¼Œæ–‡æ¡£æ·»åŠ æ“ä½œå¤±è´¥")
            return []

        total_documents = len(documents)
        if total_documents == 0:
            logger.warning(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] è­¦å‘Š: ä¼ å…¥çš„æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œé›†åˆ '{collection_name}' æ— éœ€å¤„ç†")
            return []

        # å†…å­˜ä½¿ç”¨è­¦å‘Šå’Œç³»ç»ŸçŠ¶æ€æ£€æŸ¥
        if total_documents > MAX_DOCUMENTS_WARNING_THRESHOLD:
            logger.warning(
                f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] å†…å­˜è­¦å‘Š: å‡†å¤‡ä¸€æ¬¡æ€§å¤„ç† {total_documents} ä¸ªæ–‡æ¡£ (è¶…è¿‡é˜ˆå€¼ {MAX_DOCUMENTS_WARNING_THRESHOLD})ï¼Œ"
                f"å»ºè®®åˆ†æ‰¹ä¸Šä¼ ä»¥é¿å…å†…å­˜æº¢å‡ºã€‚å½“å‰å†…å­˜æ‰¹æ¬¡å¤§å°: {self.memory_batch_size}"
            )

        logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] å¼€å§‹å¤„ç†: é›†åˆ='{collection_name}', æ€»æ–‡æ¡£æ•°={total_documents}")
        
        all_doc_ids = []
        total_failed = 0

        # ç»Ÿä¸€ä½¿ç”¨å†…å­˜å®‰å…¨çš„æµå¼å¤„ç†å™¨
        logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] ä½¿ç”¨å†…å­˜å®‰å…¨æµå¼å¤„ç†å™¨: {total_documents} ä¸ªæ–‡æ¡£")
        batch_doc_ids, batch_failed = await self._process_documents_streaming_v2(
            documents, collection_name, vecdb, 
            rpm_limit=DEFAULT_RPM_LIMIT,
            max_memory_mb=self.max_memory_limit_mb
        )
        all_doc_ids.extend(batch_doc_ids)
        total_failed += batch_failed

        # ä¿å­˜ç´¢å¼•
        try:
            logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] å¼€å§‹ä¿å­˜é›†åˆ '{collection_name}' çš„ç´¢å¼•æ–‡ä»¶...")
            await vecdb.embedding_storage.save_index()
            logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] é›†åˆ '{collection_name}' ç´¢å¼•æ–‡ä»¶ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] ç´¢å¼•ä¿å­˜å¤±è´¥: é›†åˆ '{collection_name}' ç´¢å¼•æ–‡ä»¶ä¿å­˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œ"
                        f"å¯èƒ½å½±å“åç»­æœç´¢åŠŸèƒ½ï¼Œé”™è¯¯è¯¦æƒ…: {str(e)}")

        # æœ€ç»ˆæ¸…ç†å’Œç»Ÿè®¡
        documents.clear()
        gc.collect()
        
        success_count = len(all_doc_ids)
        success_rate = (success_count / total_documents * 100) if total_documents > 0 else 0
        
        if total_failed == 0:
            logger.info(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] âœ… ä»»åŠ¡å®Œæˆ: é›†åˆ '{collection_name}' æˆåŠŸæ·»åŠ  {success_count}/{total_documents} ä¸ªæ–‡æ¡£ (100%)")
        else:
            logger.warning(f"[çŸ¥è¯†åº“-æ·»åŠ æ–‡æ¡£] âš ï¸ ä»»åŠ¡å®Œæˆ(æœ‰å¤±è´¥): é›†åˆ '{collection_name}' æˆåŠŸ {success_count}/{total_documents} ä¸ªæ–‡æ¡£ "
                          f"({success_rate:.1f}%), å¤±è´¥ {total_failed} ä¸ªï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹é”™è¯¯æ—¥å¿—")
        
        return all_doc_ids

    async def search(
        self, collection_name: str, query_text: str, top_k: int = 5
    ) -> List[Tuple[Document, float]]:
        logger.info(f"[çŸ¥è¯†åº“-æœç´¢] å¼€å§‹æœç´¢: é›†åˆ='{collection_name}', æŸ¥è¯¢æ–‡æœ¬é¢„è§ˆ='{query_text[:30]}...', top_k={top_k}")
        
        if not await self.collection_exists(collection_name):
            logger.warning(f"[çŸ¥è¯†åº“-æœç´¢] è­¦å‘Š: Faissé›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œæœç´¢ç»“æœä¸ºç©º")
            return []

        # é¦–å…ˆå¤„ç†æ—§é›†åˆ
        if collection_name in self._old_collections:
            if self._old_faiss_store:
                logger.info(f"[çŸ¥è¯†åº“-æœç´¢] ä½¿ç”¨æ—§å­˜å‚¨å¼•æ“å¤„ç†é›†åˆ '{collection_name}' çš„æœç´¢")
                return await self._old_faiss_store.search(
                    collection_name, query_text, top_k
                )
            else:
                logger.error(
                    f"[çŸ¥è¯†åº“-æœç´¢] é”™è¯¯: æ—§é›†åˆ '{collection_name}' å­˜åœ¨ä½† OldFaissStore æœªåˆå§‹åŒ–"
                )
                return []

        # è·å–æˆ–åŠ è½½é›†åˆå®ä¾‹
        logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] è·å–é›†åˆ '{collection_name}' çš„å‘é‡æ•°æ®åº“å®ä¾‹")
        vecdb = await self._get_or_load_vecdb(collection_name)
        if not vecdb:
            logger.error(f"[çŸ¥è¯†åº“-æœç´¢] é”™è¯¯: æ— æ³•è·å–æˆ–åŠ è½½é›†åˆ '{collection_name}' çš„å‘é‡æ•°æ®åº“å®ä¾‹ï¼Œæœç´¢å¤±è´¥")
            return []

        try:
            # å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é‡æ’åºï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
            has_rerank_attr = hasattr(vecdb, 'rerank_provider')
            if has_rerank_attr:
                rerank_provider_value = getattr(vecdb, 'rerank_provider', None)
                use_rerank = rerank_provider_value is not None
                logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] FaissVecDBæœ‰rerank_providerå±æ€§ï¼Œå€¼: {rerank_provider_value is not None}")
            else:
                use_rerank = False
                logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] FaissVecDBæ²¡æœ‰rerank_providerå±æ€§ï¼Œä½¿ç”¨æ™®é€šæœç´¢æ¨¡å¼")
                
            if use_rerank:
                # å¯¹äºæœ‰é‡æ’åºçš„æƒ…å†µï¼Œå…ˆæ£€ç´¢æ›´å¤šç»“æœå†é‡æ’åº
                logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] ä½¿ç”¨é‡æ’åºæ¨¡å¼æœç´¢ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {max(20, top_k)}")
                results = await vecdb.retrieve(
                    query=query_text,
                    k=max(20, top_k)
                )
                # æ‰‹åŠ¨é‡æ’åºï¼ˆå¦‚æœæœ‰é‡æ’åºæä¾›å•†çš„è¯ï¼‰
                if hasattr(vecdb, 'rerank_provider') and vecdb.rerank_provider:
                    try:
                        # æå–æ–‡æ¡£æ–‡æœ¬ç”¨äºé‡æ’åº
                        documents = [result.data.get("text", "") for result in results if result]
                        reranked_results = await vecdb.rerank_provider.rerank(query_text, documents)
                        # é‡æ–°æ’åºç»“æœ
                        if reranked_results:
                            reranked_indices = [item.index for item in reranked_results[:top_k]]
                            results = [results[i] for i in reranked_indices if i < len(results)]
                        else:
                            results = results[:top_k]
                        logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(results)}")
                    except Exception as rerank_e:
                        logger.warning(f"[çŸ¥è¯†åº“-æœç´¢] é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {str(rerank_e)}")
                        results = results[:top_k]
                else:
                    results = results[:top_k]
            else:
                logger.debug(f"[çŸ¥è¯†åº“-æœç´¢] ä½¿ç”¨æ™®é€šæ¨¡å¼æœç´¢ï¼Œæ£€ç´¢æ•°é‡: {top_k}")
                results = await vecdb.retrieve(query=query_text, k=top_k)
                
        except Exception as e:
            logger.error(f"[çŸ¥è¯†åº“-æœç´¢] æœç´¢å¼‚å¸¸: åœ¨é›†åˆ '{collection_name}' ä¸­æœç´¢æ—¶å‘ç”Ÿé”™è¯¯ï¼Œ"
                        f"é”™è¯¯ç±»å‹: {type(e).__name__}ï¼Œé”™è¯¯è¯¦æƒ…: {str(e)}")
            return []

        # å¤„ç†æœç´¢ç»“æœ
        ret = []
        failed_parse_count = 0
        for i, result in enumerate(results):
            if result is not None:
                try:
                    metadata = json.loads(result.data.get("metadata", "{}"))
                except json.JSONDecodeError as json_e:
                    failed_parse_count += 1
                    metadata = {}
                    logger.warning(
                        f"[çŸ¥è¯†åº“-æœç´¢] JSONè§£æå¤±è´¥: é›†åˆ {collection_name} æ–‡æ¡£ {result.data.get('doc_id')} å…ƒæ•°æ®è§£æå¤±è´¥ï¼Œé”™è¯¯: {str(json_e)}"
                    )
                doc = Document(
                    id=result.data.get("doc_id"),
                    embedding=[],  # åŸå§‹ä»£ç è¿™é‡Œå°±æ˜¯ç©º
                    text_content=result.data.get("text", ""),
                    metadata=metadata,
                )
                ret.append((doc, result.similarity))
                
        # è¯¦ç»†çš„æœç´¢ç»“æœæ—¥å¿—
        if len(ret) == 0:
            logger.warning(f"[çŸ¥è¯†åº“-æœç´¢] æœç´¢ç»“æœä¸ºç©º: é›†åˆ '{collection_name}' ä¸­æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ '{query_text[:30]}...' ç›¸å…³çš„å†…å®¹")
        else:
            avg_similarity = sum(score for _, score in ret) / len(ret)
            logger.info(
                f"[çŸ¥è¯†åº“-æœç´¢] âœ“ æœç´¢å®Œæˆ: é›†åˆ='{collection_name}', æŸ¥è¯¢='{query_text[:30]}...', "
                f"è¿”å›ç»“æœæ•°={len(ret)}, å¹³å‡ç›¸ä¼¼åº¦={avg_similarity:.3f}"
                + (f", JSONè§£æå¤±è´¥={failed_parse_count}ä¸ª" if failed_parse_count > 0 else "")
            )
        return ret

    async def delete_collection(self, collection_name: str) -> bool:
        if not await self.collection_exists(collection_name):
            logger.info(f"Faiss é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")
            return False

        # é¦–å…ˆå¤„ç†æ—§é›†åˆ
        if collection_name in self._old_collections:
            self._old_collections.pop(collection_name, None)
            if self._old_faiss_store:
                return await self._old_faiss_store.delete_collection(collection_name)
            return False

        # å¦‚æœé›†åˆåœ¨ç¼“å­˜ä¸­ï¼Œå…ˆå¸è½½å¹¶å…³é—­å®ƒ
        await self._unload_collection(collection_name)
        # ä»å·²çŸ¥é›†åˆåˆ—è¡¨ä¸­ç§»é™¤
        self._all_known_collections.discard(collection_name)

        # ä¿æŒæ–‡ä»¶åˆ é™¤åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œ
        def _delete_sync():
            # self.vecdbs.pop(collection_name, None) # æ”¹ä¸º _unload_collection
            _, file_id, index_path, storage_path, _ = self._get_collection_meta(
                collection_name
            )

            try:
                if os.path.exists(index_path):
                    os.remove(index_path)
                if os.path.exists(storage_path):
                    os.remove(storage_path)
                logger.info(
                    f"Faiss é›†åˆæ–‡ä»¶ '{collection_name}' (file_id: {file_id}) å·²åˆ é™¤ã€‚"
                )
                return True
            except Exception as e:
                logger.error(f"åˆ é™¤ Faiss é›†åˆ '{collection_name}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                return False

        return await asyncio.to_thread(_delete_sync)

    async def list_collections(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²çŸ¥çš„é›†åˆï¼ˆåŒ…æ‹¬ç¼“å­˜ä¸­çš„ã€ç£ç›˜ä¸ŠæœªåŠ è½½çš„ã€æ—§æ ¼å¼çš„ï¼‰"""
        # é‡æ–°æ‰«æå¯èƒ½æ›´å‡†ç¡®ï¼Œä½†ä¸ºäº†æ•ˆç‡ï¼Œä¾èµ–åˆå§‹åŒ–æ‰«æå’Œåˆ›å»º/åˆ é™¤æ—¶çš„ç»´æŠ¤
        # await self._scan_collections_on_disk()
        return list(self._all_known_collections) + list(self._old_collections.keys())

    async def count_documents(self, collection_name: str) -> int:
        if not await self.collection_exists(collection_name):
            return 0
        # é¦–å…ˆå¤„ç†æ—§é›†åˆ
        if collection_name in self._old_collections:
            if self._old_faiss_store:
                return await self._old_faiss_store.count_documents(collection_name)
            else:
                return 0

        # è·å–æˆ–åŠ è½½é›†åˆå®ä¾‹
        vecdb = await self._get_or_load_vecdb(collection_name)
        if not vecdb:
            logger.warning(f"æ— æ³•è·å–æˆ–åŠ è½½é›†åˆ '{collection_name}' æ¥è®¡æ•°ã€‚")
            return 0
        try:
            cnt = await vecdb.count_documents()
            return cnt
        except Exception as e:
            logger.error(f"è·å–é›†åˆ '{collection_name}' æ–‡æ¡£æ•°é‡æ—¶å‡ºé”™: {e}")
            return 0

    async def close(self):
        """å…³é—­æ‰€æœ‰ç¼“å­˜ä¸­çš„é›†åˆå’Œæ—§å­˜å‚¨"""
        logger.info(f"æ­£åœ¨å…³é—­æ‰€æœ‰å·²åŠ è½½çš„ Faiss é›†åˆ (ç¼“å­˜å¤§å°: {len(self.cache)})...")
        # å¤åˆ¶ key åˆ—è¡¨ï¼Œå› ä¸º _unload_collection ä¼šä¿®æ”¹ self.cache
        try:
            collections_to_unload = list(self.cache.keys())
            for collection_name in collections_to_unload:
                await self._unload_collection(collection_name)

            self.cache.clear()
            self.embedding_utils.clear()
            self._locks.clear()
            self._all_known_collections.clear()
            logger.info("æ‰€æœ‰ç¼“å­˜ä¸­çš„ Faiss é›†åˆå·²å…³é—­å’Œæ¸…ç†ã€‚")

            if self._old_faiss_store:
                logger.info("æ­£åœ¨å…³é—­ OldFaissStore...")
                await self._old_faiss_store.close()
                self._old_faiss_store = None
                self._old_collections.clear()
                logger.info("OldFaissStore å·²å…³é—­ã€‚")

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            logger.debug("å·²è§¦å‘åƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜")

        except Exception as e:
            logger.error(f"å…³é—­ Faiss é›†åˆæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        logger.info("FaissStore å…³é—­å®Œæˆã€‚")
